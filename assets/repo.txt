src/visionlab/predict.py
---
from pathlib import Path

import hydra
import lightning as L
import structlog
import torch
from omegaconf import DictConfig, OmegaConf

from visionlab.utils.misc import log_useful_info, set_seed
from visionlab.utils.registry import load_obj

log = structlog.get_logger()


def _predict(
    model: L.LightningModule,
    dm: L.LightningDataModule,
    cfg: DictConfig,
):
    dm.prepare_data()
    dm.setup(stage="predict")
    dl = dm.predict_dataloader()

    model.eval()
    with torch.no_grad():
        for idx, inputs in enumerate(dl):
            images, preds = model.predict_step(inputs, idx)
            yield dm.visualize_batch(images, preds)


def _run(cfg: DictConfig) -> None:
    set_seed(cfg.training.seed)
    log.info("**** Running predict func ****")
    root_dir = Path(cfg.general.root_dir).joinpath(
        cfg.general.exp_name,
    )

    model = load_obj(cfg.training.lit_model.class_name).load_from_checkpoint(
        root_dir.joinpath(cfg.predict.checkpoint),
        cfg=cfg,
    )

    dm = load_obj(cfg.datamodule.class_name)(cfg=cfg)
    return _predict(model, dm, cfg)


@hydra.main(
    version_base=None,
    config_path="conf",
    config_name="config",
)
def run(cfg: DictConfig) -> None:
    Path("logs").mkdir(exist_ok=True)
    log.info(OmegaConf.to_yaml(cfg))
    if cfg.general.log_code:
        log_useful_info()
    _run(cfg)


if __name__ == "__main__":
    run()


---
src/visionlab/__init__.py
---
import logging
import sys

import structlog
from structlog.processors import CallsiteParameter

# Structlog configuration
logging.basicConfig(
    format="%(message)s",
    stream=sys.stdout,
    level=logging.INFO,
)

# Disable uvicorn logging
logging.getLogger("uvicorn.error").disabled = True
logging.getLogger("uvicorn.access").disabled = True
logging.getLogger("lightning").propagate = False


structlog.configure(
    processors=[
        structlog.contextvars.merge_contextvars,
        structlog.processors.add_log_level,
        structlog.processors.StackInfoRenderer(),
        structlog.processors.ExceptionRenderer(),
        structlog.dev.set_exc_info,
        structlog.processors.CallsiteParameterAdder(
            [
                CallsiteParameter.FUNC_NAME,
                CallsiteParameter.LINENO,
            ],
        ),
        structlog.processors.TimeStamper(fmt="iso", utc=True),
        structlog.dev.ConsoleRenderer()
        # structlog.processors.JSONRenderer(),
    ],
    logger_factory=structlog.PrintLoggerFactory(),
)


---
src/visionlab/notes.md
---
- All stems receive and return input as `[n, c, h, w]` format.
- All stems have the func: get_shape which does a forward pass and returns shape.
- Stem `expansions` get multiplied with the `out_channels`.
- `expansions` in other blocks like trunk, head, work on `in_channels`.
- T2T arch works well with narrow and deep transformer blocks.
    - `embed_sz=64`, `n_layers=12`
- CCT arch works well with wide and shallow(er) transformer blocks.
    - `embed_sz=128`, `n_layers=4`


---
src/visionlab/train.py
---
from pathlib import Path

import hydra
import lightning as L
import structlog
import torch
from omegaconf import DictConfig, OmegaConf

from visionlab.utils.misc import log_useful_info, set_seed
from visionlab.utils.registry import load_obj

log = structlog.get_logger()
torch.set_float32_matmul_precision("medium")


def _run(cfg: DictConfig) -> None:
    set_seed(cfg.training.seed)
    log.info("**** Running train func ****")

    exp_name = cfg.general.exp_name
    root_dir = Path(cfg.general.root_dir).joinpath(exp_name)

    loggers = []
    if cfg.logging.log:
        for logger in cfg.logging.loggers:
            loggers.append(load_obj(logger.class_name)(**logger.params))

    callbacks = []
    for callback_name, callback in cfg.callback.items():
        if callback_name in ["model_checkpoint", "vis_dls"]:
            dirpath = root_dir.joinpath(callback.params.dirpath)
            callback.params.dirpath = dirpath.as_posix()
        callback_instance = load_obj(callback.class_name)(**callback.params)
        callbacks.append(callback_instance)

    trainer = L.Trainer(
        logger=loggers,
        callbacks=callbacks,
        **cfg.training.trainer_params,
    )
    model = load_obj(cfg.training.lit_model.class_name)(cfg=cfg)
    dm = load_obj(cfg.datamodule.class_name)(cfg=cfg)
    trainer_kwargs = {}
    if cfg.training.get("resume") is not None:
        ckpt_dir = Path(cfg.callback.model_checkpoint.params.dirpath)
        ckpt_path = ckpt_dir.joinpath(cfg.training.resume.checkpoint).as_posix()
        log.info(f"Resuming from ckpt: {ckpt_path}")
        trainer_kwargs["ckpt_path"] = ckpt_path
    trainer.fit(model, dm, **trainer_kwargs)
    trainer.test(model, dm, ckpt_path="best")
    log.info(f"{root_dir = }")


@hydra.main(
    version_base=None,
    config_path="conf",
    config_name="config",
)
def run(cfg: DictConfig) -> None:
    Path("logs").mkdir(exist_ok=True)
    log.info(OmegaConf.to_yaml(cfg))
    if cfg.general.log_code:
        log_useful_info()
    _run(cfg)


if __name__ == "__main__":
    run()


---
src/visionlab/augs/albumentations_aug.py
---
import albumentations as A
from visionlab.utils.registry import load_obj
from omegaconf import DictConfig
from omegaconf.listconfig import ListConfig


def load_augs(cfg: DictConfig) -> A.Compose:
    """
    Load albumentations

    Args:
        cfg:

    Returns:
        compose object
    """
    augs = []
    for a in cfg:
        if a.class_name == "albumentations.OneOf":
            small_augs = []
            for small_aug in a.params:
                # yaml can't contain tuples, so we need to convert manually
                params = {
                    k: (v if type(v) is not ListConfig else tuple(v))
                    for k, v in small_aug.params.items()
                }
                aug = load_obj(small_aug.class_name)(**params)
                small_augs.append(aug)
            aug = load_obj(a.class_name)(small_augs)
            augs.append(aug)

        else:
            params = {
                k: (v if type(v) is not ListConfig else tuple(v))
                for k, v in a.params.items()
            }
            aug = load_obj(a.class_name)(**params)
            augs.append(aug)

    return A.Compose(augs)


---
src/visionlab/datamodules/parser_factory.py
---
from visionlab.datamodules.folder_structs import FolderStructs


class FolderParserFactory:
    @staticmethod
    def get_folder_parser(folder_struct: str):
        return FolderStructs[folder_struct]


---
src/visionlab/datamodules/image_clf_dm.py
---
import lightning as L
from visionlab.augs.albumentations_aug import load_augs
from visionlab.datamodules.parser_factory import FolderParserFactory
from visionlab.utils.registry import load_obj
from omegaconf import DictConfig
from torch.utils.data import DataLoader


class ImageClfDatamodule(L.LightningDataModule):
    def __init__(self, cfg: DictConfig):
        super().__init__()
        self.cfg = cfg
        self.path = cfg.datamodule.path
        self.dataset_cls = load_obj(self.cfg.datamodule.dataset.class_name)
        self.train_augs = load_augs(self.cfg.augmentation.train)
        self.val_augs = load_augs(self.cfg.augmentation.val)

        self.folder_struct = cfg.datamodule.folder_struct.upper()
        self.folder_struct_parser = FolderParserFactory.get_folder_parser(
            self.folder_struct
        ).value(path_src=self.path, **cfg.datamodule.parser_params)

    def prepare_data(self):
        pass

    def setup(self, stage=None):
        class_names = self.folder_struct_parser.class_names
        class_to_idx = self.folder_struct_parser.class_to_idx

        train_samples = self.folder_struct_parser.get_train()
        val_samples = self.folder_struct_parser.get_val()
        test_samples = self.folder_struct_parser.get_test()

        if self.cfg.training.debug:
            train_samples = train_samples[:100]
            val_samples = val_samples[:100]
            test_samples = test_samples[:100]

        if stage == "fit" or stage is None:
            self.train_dataset = self.dataset_cls(
                samples=train_samples,
                mode="train",
                classes=class_names,
                class_to_idx=class_to_idx,
                transform=self.train_augs,
            )

            self.val_dataset = self.dataset_cls(
                samples=val_samples,
                mode="val",
                classes=class_names,
                class_to_idx=class_to_idx,
                transform=self.val_augs,
            )

        if stage in ["test", "predict"] or stage is None:
            self.test_dataset = self.dataset_cls(
                samples=test_samples,
                mode="test",
                classes=class_names,
                class_to_idx=class_to_idx,
                transform=self.val_augs,
            )

    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.cfg.datamodule.batch_size,
            num_workers=self.cfg.datamodule.num_workers,
            pin_memory=self.cfg.datamodule.pin_memory,
            shuffle=True,
            drop_last=True,
        )

    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.cfg.datamodule.batch_size,
            num_workers=self.cfg.datamodule.num_workers,
            pin_memory=self.cfg.datamodule.pin_memory,
            shuffle=False,
            drop_last=False,
        )

    def test_dataloader(self):
        bs = self.cfg.datamodule.test_batch_size if self.cfg.training.debug else 4
        return DataLoader(
            self.test_dataset,
            batch_size=bs,
            num_workers=self.cfg.datamodule.num_workers,
            pin_memory=self.cfg.datamodule.pin_memory,
            shuffle=False,
            drop_last=False,
        )

    def predict_dataloader(self):
        bs = self.cfg.datamodule.test_batch_size if self.cfg.training.debug else 4
        return DataLoader(
            self.test_dataset,
            batch_size=bs,
            num_workers=self.cfg.datamodule.num_workers,
            pin_memory=self.cfg.datamodule.pin_memory,
            shuffle=True,
            drop_last=False,
        )


---
src/visionlab/datamodules/folder_structs.py
---
from enum import Enum

from visionlab.datamodules.parsers.cls_subfolder import ClassSubfolderParser
from visionlab.datamodules.parsers.splits_subfolder import (
    SplitsSubfolderParser,
)


class FolderStructs(Enum):
    CLS_SUBFOLDER = ClassSubfolderParser
    TRAIN_VAL_TEST = SplitsSubfolderParser


---
src/visionlab/datamodules/tv_clf_dm.py
---
import lightning as L
import numpy as np
import structlog
import torch
from visionlab.augs.albumentations_aug import load_augs
from visionlab.utils.registry import load_obj
from visionlab.utils.vis import draw_labels_on_images
from omegaconf import DictConfig
from torch.utils.data import DataLoader, random_split
from torch.utils.data._utils.collate import default_collate

log = structlog.get_logger()


class TorchvisionClfDatamodule(L.LightningDataModule):
    def __init__(self, cfg: DictConfig) -> None:
        super().__init__()
        self.cfg = cfg
        self.path = cfg.datamodule.path
        self.name = cfg.datamodule.dataset.name
        self.splits = cfg.datamodule.splits
        self.dataset_cls = load_obj(cfg.datamodule.dataset.class_name)
        self.train_augs = load_augs(cfg.augmentation.train)
        self.val_augs = load_augs(cfg.augmentation.val)
        self.train_collate_fn = self.get_collate_fn(self.train_augs)
        self.val_collate_fn = self.get_collate_fn(self.val_augs)

    def get_collate_fn(self, transform):
        def collate_fn(batch):
            [
                sample.update(
                    {
                        "image": transform(
                            image=sample["image"],
                        )["image"]
                    }
                )
                for sample in batch
            ]
            return default_collate(batch)

        return collate_fn

    def prepare_data(self):
        self.dataset_cls(
            self.name,
            root=self.path,
            train=True,
            download=True,
        )
        self.dataset_cls(
            self.name,
            root=self.path,
            train=False,
            download=True,
        )

    def setup(self, stage=None):
        if stage == "fit" or stage is None:
            self.train_dataset = self.dataset_cls(
                self.name,
                root=self.path,
                train=True,
            )

            self.val_dataset = self.dataset_cls(
                self.name,
                root=self.path,
                train=False,
            )
            self.classes = self.train_dataset.classes

            if self.cfg.training.debug:
                self.train_dataset, _ = random_split(
                    self.train_dataset,
                    [0.1, 0.9],
                )
                self.val_dataset, _ = random_split(
                    self.val_dataset,
                    [0.1, 0.9],
                )
            log.info(f"train_dataset size: {len(self.train_dataset)}")
            log.info(f"val_dataset size: {len(self.val_dataset)}")

        if stage in ["fit", "test", "predict"] or stage is None:
            self.test_dataset = self.dataset_cls(
                self.name,
                root=self.path,
                train=False,
            )
            self.classes = self.test_dataset.classes

    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.cfg.datamodule.batch_size,
            num_workers=self.cfg.datamodule.num_workers,
            pin_memory=self.cfg.datamodule.pin_memory,
            collate_fn=self.train_collate_fn,
            shuffle=True,
            drop_last=True,
        )

    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.cfg.datamodule.batch_size,
            num_workers=self.cfg.datamodule.num_workers,
            pin_memory=self.cfg.datamodule.pin_memory,
            collate_fn=self.val_collate_fn,
            shuffle=False,
            drop_last=False,
        )

    def test_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_size=self.cfg.datamodule.test_batch_size,
            num_workers=self.cfg.datamodule.num_workers,
            collate_fn=self.val_collate_fn,
            pin_memory=False,
            shuffle=False,
            drop_last=False,
        )

    def predict_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_size=self.cfg.datamodule.test_batch_size,
            num_workers=self.cfg.datamodule.num_workers,
            collate_fn=self.val_collate_fn,
            pin_memory=False,
            shuffle=True,
            drop_last=False,
        )

    def visualize_batch(self, inputs, outputs=None, num_samples=8):
        if outputs is None:
            outputs = inputs["target"]
            inputs = inputs["image"]
        outputs_resolved = np.array(self.classes)[outputs.detach().cpu()]

        std = torch.tensor(self.cfg.datamodule.dataset.std).view(3, 1, 1)
        mean = torch.tensor(self.cfg.datamodule.dataset.mean).view(3, 1, 1)
        images = inputs.detach().cpu() * std + mean

        return draw_labels_on_images(
            images[:num_samples],
            outputs_resolved[:num_samples],
            20,
            size=120,
        )


---
src/visionlab/datamodules/parsers/cls_subfolder.py
---
import random
from pathlib import Path
from typing import List

from visionlab.utils.misc import split_list


class ClassSubfolderParser:
    """
    dataset/
    ├── class1/
    │   ├── image1.jpg
    │   ├── image2.jpg
    │   └── ...
    ├── class2/
    │   ├── image1.jpg
    │   ├── image2.jpg
    │   └── ...
    └── ...
    """

    def __init__(
        self,
        path_src: str,
        train_val_test_split: List[float] = [0.7, 0.2, 0.1],
    ):
        self.path_src = Path(path_src)
        self.train_val_test_split = train_val_test_split
        self.class_names = self._get_class_names()
        self.class_to_idx = {
            class_name: i for i, class_name in enumerate(self.class_names)
        }
        all_samples = self._gather_samples()
        (
            self.train_samples,
            self.val_samples,
            self.test_samples,
        ) = split_list(all_samples, train_val_test_split)

    def _get_class_names(self) -> List[str]:
        return sorted(
            [
                d.name
                for d in self.path_src.iterdir()
                if d.is_dir() and not d.name.startswith(".")
            ]
        )

    def _gather_samples(self):
        samples = []
        for class_name in self.class_names:
            class_path = self.path_src.joinpaht(class_name)
            for image_path in class_path.iterdir():
                if image_path.suffix in [".jpg", ".jpeg", ".png", ".bmp"]:
                    samples.append(
                        (
                            image_path,
                            self.mapping_dict[class_name],
                        )
                    )
        return samples

    def get_train(self, shuffle=True):
        if shuffle:
            random.shuffle(self.train_samples)
        self.train_samples

    def get_val(self):
        return self.val_samples

    def get_test(self):
        return self.test_samples


---
src/visionlab/datamodules/parsers/splits_subfolder.py
---
import random
from pathlib import Path
from typing import List, Tuple


class SplitsSubfolderParser:
    """
    dataset/
    ├── train/
    │   ├── class1/
    │   │   ├── image1.jpg
    │   │   └── ...
    │   ├── class2/
    │   │   ├── image1.jpg
    │   │   └── ...
    │   └── ...
    ├── val/
    │   ├── class1/
    │   │   ├── image1.jpg
    │   │   └── ...
    │   ├── class2/
    │   │   ├── image1.jpg
    │   │   └── ...
    │   └── ...
    └── test/
        ├── class1/
        │   ├── image1.jpg
        │   └── ...
        ├── class2/
        │   ├── image1.jpg
        │   └── ...
        └── ...
    """

    def __init__(self, path_src: str):
        self.path_src = Path(path_src)
        self.class_names = self._get_class_names()
        self.class_to_idx = {
            cls_name: idx for idx, cls_name in enumerate(self.class_names)
        }

    def _get_class_names(self) -> List[str]:
        train_dir = self.path_src.joinpath("train")
        class_names = sorted(
            [
                d.name
                for d in train_dir.iterdir()
                if d.is_dir() and not d.name.startswith(".")
            ]
        )
        return class_names

    def _gather_samples(self, split: str) -> List[Tuple[Path, int]]:
        split_dir = self.path_src.joinpath(split)
        samples = []

        for class_idx, class_name in enumerate(self.class_names):
            class_dir = split_dir.joinpath(class_name)
            for image_path in class_dir.iterdir():
                if image_path.suffix.lower() in [".png", ".jpg", ".jpeg", ".bmp"]:
                    samples.append((image_path, class_idx))

        return samples

    def get_train(self, shuffle=True) -> List[Tuple[Path, int]]:
        train_samples = self._gather_samples("train")
        if shuffle:
            random.shuffle(train_samples)
        return train_samples

    def get_val(self) -> List[Tuple[Path, int]]:
        return self._gather_samples("valid")

    def get_test(self) -> List[Tuple[Path, int]]:
        return self._gather_samples("test")


---
src/visionlab/utils/ml.py
---
from typing import Any, Optional

import torch
from torch import nn


def freeze_until(net: Any, param_name: Optional[str]) -> None:
    """
    Freeze net until param_name

    https://opendatascience.slack.com/archives/CGK4KQBHD/p1588373239292300?thread_ts=1588105223.275700&cid=CGK4KQBHD

    Args:
        net:
        param_name:

    """
    found_name = False
    for name, params in net.named_parameters():
        if name == param_name:
            found_name = True
        params.requires_grad = found_name
    return found_name


def count_parameters(model: Any):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def swap_dims(x, d1, d2):
    # x: torch.tensor, d1 and d2: dims to swap
    dims = list(range(x.dim()))
    dims[d1], dims[d2] = dims[d2], dims[d1]
    return x.permute(dims)


def init_linear(m):
    if isinstance(m, (nn.Linear, nn.Conv2d)):
        nn.init.xavier_normal_(m.weight)
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, nn.BatchNorm2d):
        nn.init.constant_(m.weight, 1.0)
        nn.init.zeros_(m.bias)
    elif isinstance(m, nn.LayerNorm):
        nn.init.constant_(m.weight, 1.0)
        nn.init.zeros_(m.bias)


def compute_output_shape(module, input_shape):
    with torch.no_grad():
        dummy_input = torch.zeros(1, *input_shape)
        output = module(dummy_input)
        return output.shape[1:]


def make_divisible(v, divisor, min_value=None):
    """
    This function is taken from the original tf repo.
    It ensures that all layers have a channel number that is divisible by 8
    It can be seen here:
    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
    """
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    # Make sure that round down does not go down by more than 10%.
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v


---
src/visionlab/utils/vis.py
---
import torch
from torchvision.transforms.functional import to_pil_image
from torchvision.transforms.v2 import Resize
from torchvision.utils import draw_bounding_boxes, make_grid


def draw_labels_on_images(images, labels, label_h=30, size=None):
    labeled_images = []
    for image, label in zip(images, labels):
        if size is not None:
            image = Resize(size=size)(image)
        text_label = f"{label}"

        black_bg = torch.zeros(
            (3, label_h, image.shape[2]),
            dtype=torch.uint8,
        )

        labeled_bg = draw_bounding_boxes(
            image=black_bg,
            boxes=torch.tensor([[0, 0, black_bg.shape[-1], label_h]]),
            labels=[text_label],
            colors=["white"],
            font="../assets/Ubuntu-R.ttf",
            fill=False,
            width=2,
            font_size=label_h * 0.7,
        )

        labeled_image = torch.cat(
            (labeled_bg, (255 * image).byte()),
            dim=1,
        )
        labeled_images.append(labeled_image)

    return torch.stack(labeled_images, dim=0)


def gridify(visualized, fp=None, scale=None, **kwargs):
    im = to_pil_image(make_grid(visualized, **kwargs))
    if scale is not None:
        og_size = im.size
        size = tuple([int(x * scale) for x in og_size])
        im = im.resize(size)
    if fp is not None:
        im.save(fp)
    else:
        return im


---
src/visionlab/utils/registry.py
---
import importlib
from collections import OrderedDict
from typing import Any

from visionlab.utils.ml import compute_output_shape
from omegaconf import DictConfig
from torch import nn


def load_obj(obj_path: str, namespace: str = "visionlab") -> Any:
    """
    Extract an object from a given path.
    https://github.com/quantumblacklabs/kedro/blob/9809bd7ca0556531fa4a2fc02d5b2dc26cf8fa97/kedro/utils.py
        Args:
            obj_path: Path to an object to be extracted, including the object name.
            namespace: Default namespace.
        Returns:
            Extracted object.
        Raises:
            AttributeError: When the object does not have the given named attribute.
    """
    obj_path_list = obj_path.rsplit(".", 1)
    obj_path = obj_path_list.pop(0) if len(obj_path_list) > 1 else ""
    obj_name = obj_path_list[0]
    try:
        module_obj = importlib.import_module(f"{obj_path}")
    except ModuleNotFoundError:
        module_obj = importlib.import_module(f"{namespace}.{obj_path}")
    if not hasattr(module_obj, obj_name):
        raise AttributeError(
            f"Object `{obj_name}` cannot be loaded from `{obj_path}`.")
    return getattr(module_obj, obj_name)


def load_module(cfg):
    layers = cfg["layers"]
    c, h, w = (cfg["ip"].get(i) for i in ("c", "h", "w"))
    input_shape = (c, h, w)
    custom_modules = "models.blocks"

    module_dict = OrderedDict()
    for layer_name, layer_config in layers.items():
        from_layer, num_repeats, block, block_args = layer_config

        if from_layer == "ip":
            input_shape = (c, h, w)
        else:
            input_shape = module_dict[from_layer].output_shape

        if block.startswith("nn."):
            block_class = getattr(nn, block.split(".", 1)[1])
        else:
            try:
                block_class = load_obj(f"{custom_modules}.{block}")
            except KeyError:
                raise ImportError(f"Block '{block}' not found.")

        pos_args = []
        kwargs = {}
        for arg in block_args:
            if isinstance(arg, DictConfig):
                kwargs.update(arg)
            else:
                pos_args.append(arg)

        module = block_class(*pos_args, **kwargs)
        module.output_shape = compute_output_shape(module, input_shape)

        blocks = [module]
        for _ in range(num_repeats - 1):
            module = block_class(*pos_args, **kwargs)
            module.output_shape = compute_output_shape(module, input_shape)
            blocks.append(module)

        stage = nn.Sequential(*blocks)
        stage.output_shape = module.output_shape
        module_dict[layer_name] = stage
    model = nn.Sequential(module_dict)
    model.output_shape = compute_output_shape(model, (c, h, w))
    return model


---
src/visionlab/utils/config.py
---
import collections
from itertools import product
from typing import Dict, Generator

from omegaconf import DictConfig, OmegaConf


def product_dict(**kwargs: Dict) -> Generator:
    """
    Convert dict with lists in values into lists of all combinations

    This is necessary to convert config with experiment values
    into format usable by hydra
    Args:
        **kwargs:

    Returns:
        list of lists

    ---
    Example:
        >>> list_dict = {'a': [1, 2], 'b': [2, 3]}
        >>> list(product_dict(**list_dict))
        >>> [['a=1', 'b=2'], ['a=1', 'b=3'], ['a=2', 'b=2'], ['a=2', 'b=3']]

    """
    keys = kwargs.keys()
    vals = kwargs.values()
    for instance in product(*vals):
        zip_list = list(zip(keys, instance))
        yield [f"{i}={j}" for i, j in zip_list]


def config_to_hydra_dict(cfg: DictConfig) -> Dict:
    """
    Convert config into dict with lists of values, where key is full name of parameter

    This fuction is used to get key names which can be used in hydra.

    Args:
        cfg:

    Returns:
        converted dict

    """
    experiment_dict = {}
    for k, v in cfg.items():
        for k1, v1 in v.items():
            experiment_dict[f"{k!r}.{k1!r}"] = v1

    return experiment_dict


def flatten_omegaconf(d, sep="_"):
    d = OmegaConf.to_container(d)

    obj = collections.OrderedDict()

    def recurse(t, parent_key=""):
        if isinstance(t, list):
            for i, _ in enumerate(t):
                recurse(t[i], parent_key + sep + str(i) if parent_key else str(i))
        elif isinstance(t, dict):
            for k, v in t.items():
                recurse(v, parent_key + sep + k if parent_key else k)
        else:
            obj[parent_key] = t

    recurse(d)
    obj = {k: v for k, v in obj.items() if isinstance(v, (int, float))}

    return obj


---
src/visionlab/utils/misc.py
---
import os
import random
from typing import Any, List

import hydra
import numpy as np
import structlog
import torch

log = structlog.get_logger()


def set_seed(seed: int = 666, precision: int = 10) -> None:
    np.random.seed(seed)
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.set_printoptions(precision=precision)


def log_useful_info() -> None:
    log.info(hydra.utils.get_original_cwd())
    log.info(os.getcwd())


def split_list(data: List[Any], percentages: List[float]) -> List[List[Any]]:
    """
    Splits a list into sublists according to the given percentages.

    Parameters:
        data (List[Any]): The list to be split.
        percentages (List[float]): A list of percentages for splitting.

    Returns:
        List[List[Any]]: A list of sublists split according to the percentages.
    """
    if not sum(percentages) == 1.0:
        raise ValueError("The sum of the percentages must be 1.0")

    data_copy = data[:]
    random.shuffle(data_copy)
    total_len = len(data_copy)

    splits = []
    start = 0
    for pct in percentages:
        end = start + int(pct * total_len)
        splits.append(data_copy[start:end])
        start = end

    return splits


---
src/visionlab/callbacks/vis_dls.py
---
from pathlib import Path

from lightning.pytorch.callbacks import Callback
from visionlab.utils.vis import gridify


class VisualizeDlsCallback(Callback):
    def __init__(
        self,
        num_samples=8,
        dirpath="dls",
        scale=None,
    ):
        super().__init__()
        self.num_samples = num_samples
        self.dirpath = Path(dirpath)
        self.scale = scale

    def on_fit_start(self, trainer, pl_module):
        dm = trainer.datamodule
        train_dls = dm.train_dataloader()
        val_dls = dm.val_dataloader()

        train_batch = next(iter(train_dls))
        val_batch = next(iter(val_dls))

        train_vis = dm.visualize_batch(
            train_batch,
            num_samples=self.num_samples,
        )
        val_vis = dm.visualize_batch(
            val_batch,
            num_samples=self.num_samples,
        )

        self.dirpath.mkdir(parents=True, exist_ok=True)
        gridify(
            train_vis,
            self.dirpath.joinpath("train.jpg"),
            scale=self.scale,
            nrow=4,
        )
        gridify(
            val_vis,
            self.dirpath.joinpath("val.jpg"),
            scale=self.scale,
            nrow=4,
        )


---
src/visionlab/models/act_factory.py
---
from torch import nn

noop = nn.Identity()


class Acts:
    @staticmethod
    def get(act: str, *args, **kwargs):
        if act == "noop":
            return noop
        if act == "relu":
            return nn.ReLU(*args, **kwargs)
        elif act == "gelu":
            return nn.GELU(*args, **kwargs)
        elif act == "sigmoid":
            return nn.Sigmoid(*args, **kwargs)
        elif act == "hsigmoid":
            return nn.Hardsigmoid(*args, **kwargs)
        elif act == "hswish":
            return nn.Hardswish(*args, **kwargs)
        else:
            raise NotImplementedError


---
src/visionlab/models/attn_factory.py
---
from visionlab.models.blocks.attentions import (
    CBAMBlock,
    ChannelAttentionBlock,
    ECABlock,
    ECABlockV2,
    SEBlock,
    SpatialAttentionBlock,
)
from torch import nn

noop = nn.Identity()


class Attentions:
    @staticmethod
    def get(attn: str, *args, **kwargs):
        if attn == "ca":
            return ChannelAttentionBlock(*args, **kwargs)
        elif attn == "sa":
            return SpatialAttentionBlock(*args, **kwargs)
        elif attn == "cbam":
            return CBAMBlock(*args, **kwargs)
        elif attn == "se":
            return SEBlock(*args, **kwargs)
        elif attn == "eca":
            return ECABlock(*args, **kwargs)
        elif attn == "ecav2":
            return ECABlockV2(*args, **kwargs)
        else:
            return noop


---
src/visionlab/models/norm_factory.py
---
from torch import nn

noop = nn.Identity()


class Norms:
    @staticmethod
    def get(norm: str, *args, **kwargs):
        if norm == "noop":
            return noop
        if norm == "bn1d":
            return nn.BatchNorm1d(*args, **kwargs)
        elif norm == "bn2d":
            return nn.BatchNorm2d(*args, **kwargs)
        elif norm == "ln":
            return nn.LayerNorm(*args, **kwargs)
        else:
            raise NotImplementedError


---
src/visionlab/models/blocks/necks.py
---
from einops.layers.torch import Reduce
from torch import einsum, nn

from .layers import (
    Conv1x1Layer,
    NormAct,
)


class AvgPool(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        act="relu",
        norm="bn2d",
        drop=0.0,
        flatten=True,
    ):
        super().__init__()
        c2 = c2 or c1
        self.block = nn.Sequential(
            NormAct(c1, norm, act="noop"),
            nn.AdaptiveAvgPool2d(1),
            Conv1x1Layer(
                c1,
                c2,
                act=act,
                norm="noop",
            )
            if c2 != c1
            else nn.Identity(),
            nn.Flatten(1) if flatten else nn.Identity(),
            nn.Dropout(drop),
        )

    def forward(self, x):
        # [x: n, c, h, w]
        return self.block(x)  # [n, d, 1, 1] or [n, d]


class PatchNorm(nn.Module):
    def __init__(self, c1):
        super().__init__()

        self.block = nn.Sequential(
            nn.LayerNorm(c1),
            Reduce("n p c -> n c", "mean"),
        )

    def forward(self, x):
        return self.block(x)


class SequencePooling(nn.Module):
    def __init__(self, c1):
        super().__init__()
        self.attn_pool = nn.Sequential(
            nn.LayerNorm(c1),
            nn.Linear(c1, 1),
        )

    def forward(self, x):
        # x: [n, p, c]
        attn_weights = self.attn_pool(x).squeeze(-1)  # [n, p]
        return einsum(
            "n p, n p c -> n c",
            attn_weights.softmax(dim=1),
            x,
        )


---
src/visionlab/models/blocks/layers.py
---
import math

import torch
import torch.nn.functional as F
from einops import rearrange
from einops.layers.torch import Rearrange
from visionlab.models.act_factory import Acts
from visionlab.models.norm_factory import Norms
from torch import nn


class Shifting(nn.Module):
    def __init__(self, shift):
        super().__init__()
        self.shift = shift

    def forward(self, x):
        x_pad = F.pad(x, (self.shift, self.shift, self.shift, self.shift))
        x_lu = x_pad[:, :, : -self.shift * 2, : -self.shift * 2]
        x_ru = x_pad[:, :, : -self.shift * 2, self.shift * 2:]
        x_lb = x_pad[:, :, self.shift * 2:, : -self.shift * 2]
        x_rb = x_pad[:, :, self.shift * 2:, self.shift * 2:]
        x_cat = torch.cat([x, x_lu, x_ru, x_lb, x_rb], dim=1)
        return x_cat


class NormAct(nn.Module):
    def __init__(self, c1, norm="bn2d", act="relu"):
        super().__init__()
        self.block = nn.Sequential(
            Norms.get(norm, c1),
            Acts.get(act),
        )

    def forward(self, x):
        return self.block(x)


class ApplyNorm(nn.Module):
    def __init__(self, norm, fn, pre=True):
        super().__init__()
        layers = [norm, fn] if pre else [fn, norm]
        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)


class BasicFCLayer(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        act="relu",
        norm="ln",
        drop=0.0,
        pre_norm=True,
    ):
        super().__init__()
        c2 = c2 or c1
        fc_layers = [
            nn.Linear(c1, c2),
            Acts.get(act),
            nn.Dropout(drop),
        ]
        self.norm = norm

        norm_dims = c1 if pre_norm else c2
        self.block = ApplyNorm(
            Norms.get(norm, norm_dims),
            nn.Sequential(*fc_layers),
            pre_norm,
        )

    def forward(self, x):
        return self.block(x)


class ExpansionFCLayer(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        f=4,
        act="relu",
        norm="ln",
        drop=0.0,
        pre_norm=True,
    ):
        super().__init__()
        c2 = c2 or c1
        self.block = ApplyNorm(
            Norms.get(norm, c1 if pre_norm else c2),
            nn.Sequential(
                BasicFCLayer(
                    c1,
                    math.ceil(c1 * f),
                    act=act,
                    drop=0.0,
                    norm="noop",
                ),
                BasicFCLayer(
                    math.ceil(c1 * f),
                    c2,
                    act="noop",
                    drop=drop,
                    norm="noop",
                ),
            ),
            pre_norm,
        )

    def forward(self, x):
        return self.block(x)


class SoftSplit(nn.Module):
    def __init__(self, c1, c2=None, k=3, s=1, proj=True):
        super().__init__()
        c2 = c2 or c1
        self.block = nn.Sequential(
            nn.Unfold(kernel_size=k, stride=s, padding=(k - 1) // 2),
            Rearrange("n c p -> n p c"),
            nn.Linear(c1 * k**2, c2) if proj else nn.Identity(),
        )

    def forward(self, x):
        # ip: [n, c, w, h], op = [n, p, c]
        # p = (h - k + 2 * padding) / s + 1)**2
        return self.block(x)


class FlattenLayer(nn.Module):
    def __init__(self, c1=None, norm="noop", pre_norm=True):
        super().__init__()
        self.block = ApplyNorm(
            Norms.get(norm, c1),
            nn.Identity(),
            pre_norm,
        )

    def forward(self, x):
        if len(x.shape) == 4:  # Input shape: [n, c, h, w]
            return self.block(rearrange(x, "n c h w -> n (h w) c"))
        elif len(x.shape) == 3:  # Input shape: [n, p, c]
            return self.block(x)
        else:
            raise ValueError(f"Unsupported input shape: {x.shape}")


class UnflattenLayer(nn.Module):
    def __init__(self, h=0):
        super().__init__()
        self.h = h

    def forward(self, x):
        if len(x.shape) == 4:  # Input shape: [n, c, h, w]
            return x
        elif len(x.shape) == 3:  # [n, p, c]
            p = x.shape[1]
            self.h = self.h or int(p**0.5)
            return rearrange(x, "n (h w) c -> n c h w", h=self.h)
        else:
            raise ValueError(f"Unsupported input shape: {x.shape}")


class ConvLayer(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        k=3,
        s=1,
        p="same",
        g=1,
        act="relu",
        norm="bn2d",
        pre_normact=False,
        **kwargs,
    ):
        super().__init__()
        c2 = c2 or c1
        if s > 1 and p == "same":
            p = (k - 1) // 2
        layers = [
            nn.Conv2d(
                c1,
                c2,
                kernel_size=k,
                padding=p,
                stride=s,
                groups=g,
                **kwargs,
            ),
            NormAct(c1 if pre_normact else c2, norm, act),
        ]
        if pre_normact:
            layers.reverse()
        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)


class DWConvLayer(ConvLayer):
    def __init__(self, c1, c2=None, k=3, s=1, **kwargs):
        assert c2 % c1 == 0, f"c2 {c2} must be divisible by c1 {c1}"
        super().__init__(c1, c2, k=k, s=s, g=c1, **kwargs)


class DWSepConvLayer(nn.Sequential):
    def __init__(self, c1, c2=None, k=3, s=1, pw_act="noop", **kwargs):
        c2 = c2 or c1
        super().__init__(
            DWConvLayer(c1, c1, k, s, **kwargs),
            Conv1x1Layer(c1, c2, act=pw_act),
        )


class Conv1x1Layer(ConvLayer):
    def __init__(self, c1, c2=None, s=1, **kwargs):
        super().__init__(c1, c2, k=1, s=s, **kwargs)


class DownsampleConvLayer(ConvLayer):
    def __init__(self, c1, c2=None, k=3, s=2, **kwargs):
        super().__init__(c1, c2, k=k, s=s, **kwargs)


class DownsamplePoolLayer(nn.Module):
    def __init__(self, c1, c2=None, pool="avg", **kwargs):
        super().__init__()
        self.conv = Conv1x1Layer(c1, c2, **kwargs)
        if pool == "avg":
            self.pool = nn.AvgPool2d(k=2, s=2)
        elif pool == "max":
            self.pool = nn.MaxPool2d(k=2, s=2)
        else:
            raise ValueError(f"Unsupported pool type: {pool}")

    def forward(self, x):
        return self.pool(self.conv(x))


class ShortcutLayer(nn.Module):
    def __init__(self, c1, c2=None, s=1):
        super().__init__()
        c2 = c2 or c1
        if c1 == c2 and s == 1:
            self.shortcut = nn.Identity()
        else:
            self.shortcut = Conv1x1Layer(c1, c2, s=s, act="noop")

    def forward(self, x):
        return self.shortcut(x)


class Shortcut3x3Layer(nn.Module):
    def __init__(self, c1, c2=None, s=1):
        super().__init__()
        c2 = c2 or c1
        if c1 == c2 and s == 1:
            self.shortcut = nn.Identity()
        else:
            self.shortcut = ConvLayer(c1, c2, s=s, act="noop")

    def forward(self, x):
        return self.shortcut(x)


class GhostLayer(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        k=3,
        s=1,
        act="relu",
        norm="bn2d",
    ):
        super().__init__()
        c2 = c2 or c1
        assert c2 % 2 == 0, "Output channels should be even"
        c_ = c2 // 2
        self.block_1 = Conv1x1Layer(c1, c_, act=act, norm=norm)
        self.block_2 = DWConvLayer(c_, c_, k=k, s=s, act=act, norm=norm)

    def forward(self, x):
        x1 = self.block_1(x)
        return torch.cat([x1, self.block_2(x1)], dim=1)


class DWSepShortcutLayer(nn.Module):
    def __init__(self, c1, c2, k=3, s=1, act="relu", norm="bn2d"):
        super().__init__()
        if c1 == c2 and s == 1:
            self.shortcut = nn.Identity()
        else:
            self.shortcut = DWSepConvLayer(
                c1,
                c2,
                k,
                s,
                act=act,
                pw_act="noop",
                norm=norm,
            )

    def forward(self, x):
        return self.shortcut(x)


class BottleneckLayer(nn.Sequential):
    def __init__(
        self,
        c1,
        c2=None,
        k=3,
        s=1,
        act="relu",
        norm="bn2d",
    ):
        c2 = c2 or c1
        super().__init__(
            ConvLayer(c1, c2, k=k, s=s, act=act, norm=norm),
            ConvLayer(c2, c2, act="noop", norm=norm),
        )


class LightBottleneckLayer(nn.Sequential):
    def __init__(
        self,
        c1,
        c2=None,
        f=0.5,
        k=3,
        s=1,
        g=1,
        act="relu",
        norm="bn2d",
    ):
        c2 = c2 or c1
        c_ = math.ceil(f * c1)
        super().__init__(
            Conv1x1Layer(
                c1,
                c_,
                act=act,
                norm=norm,
            )
            if c_ != c1
            else nn.Identity(),
            ConvLayer(c_, c_, k, s, g=g, act=act, norm=norm),
            Conv1x1Layer(c_, c2, act="noop", norm=norm),
        )


class DWLightBottleneckLayer(LightBottleneckLayer):
    def __init__(
        self,
        c1,
        c2=None,
        f=4,
        k=3,
        s=1,
        act="relu",
        norm="bn2d",
    ):
        g = math.ceil(f * c1)
        super().__init__(c1, c2, f, k, s, g, act, norm)


class DenseShortcutLayer(nn.Module):
    def __init__(
        self,
        c1,
        f=32,
        act="relu",
        norm="bn2d",
    ):
        super().__init__()
        self.block = ConvLayer(c1, f, act=act, norm=norm, pre_normact=True)

    def forward(self, x):
        out = self.block(x)
        return torch.cat([x, out], dim=1)


class ScaledResidual(nn.Module):
    def __init__(self, *layers, shortcut=None, apply_shortcut=True):
        super().__init__()
        self.shortcut = nn.Identity() if shortcut is None else shortcut
        self.residual = nn.Sequential(*layers)
        self.apply_shortcut = int(apply_shortcut)
        self.gamma = nn.Parameter(torch.zeros(1)) if apply_shortcut else 1

    def forward(self, x):
        return self.apply_shortcut * self.shortcut(x) + self.gamma * self.residual(x)


---
src/visionlab/models/blocks/posenc.py
---
import torch
from torch import nn


class LearnablePositionEnc(nn.Module):
    def __init__(self, sizes):
        super().__init__()
        self.pos_enc = nn.Parameter(torch.zeros(1, *sizes))
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Parameter):
            nn.init.trunc_normal_(m, std=0.2)

    def forward(self, x):
        return x + self.pos_enc


---
src/visionlab/models/blocks/__init__.py
---
from .attentions import (
    MultiScaleSAV2,
)
from .convs import (
    ConvMixerBlock,
    GhostBottleneckBlock,
    LightResNetBlock,
    MobileNetBlock,
    ResNetBlock,
    ResNeXtBlock,
)
from .heads import (
    ConvHead,
    FCHead,
)
from .layers import (
    BasicFCLayer,
    ConvLayer,
    FlattenLayer,
    SoftSplit,
    UnflattenLayer,
)
from .necks import (
    AvgPool,
    PatchNorm,
    SequencePooling,
)
from .posenc import (
    LearnablePositionEnc,
)
from .stems import (
    SPT,
    ConvMpPatch,
)
from .transformers import (
    T2TBlock,
    TransformerEncoder,
    TransformerEncoderMultiScale,
)


---
src/visionlab/models/blocks/stems.py
---
import torch.nn as nn
from einops.layers.torch import Rearrange

from .layers import (
    BasicFCLayer,
    ConvLayer,
    Shifting,
    SoftSplit,
    UnflattenLayer,
)
from .transformers import (
    T2TBlock,
)


class T2TPatchV2(nn.Module):
    def __init__(
        self,
        c1,
        c2,
        h,
        f=1,
        kn=3,
        sn=1,
        heads=1,
        act="gelu",
        drop=0.0,
        **kwargs,
    ):
        super().__init__()
        layers = []
        last = len(sn) - 1
        c_ = int(f * c2)

        for i, (k, s) in enumerate(zip(kn, sn)):
            layers.extend(
                [
                    SoftSplit(c1, c2 if i == last else c_, k, s)
                    if i == 0
                    else T2TBlock(
                        c_,
                        c2 if i == last else c_,
                        h,
                        f=f,
                        k=k,
                        s=s,
                        heads=heads,
                        act=act,
                        drop=drop,
                    ),
                ]
            )
            h //= s
        layers.append(UnflattenLayer(h=h))
        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)


class MDMLPPatch(nn.Module):
    def __init__(
        self,
        c1,
        c2,
        k,
        s,
        act="noop",
        norm="noop",
        drop=0.0,
        **kwargs,
    ):
        super().__init__()
        self.block = nn.Sequential(
            nn.Unfold(kernel_size=k, stride=s, padding=(k - 1) // 2),
            Rearrange(
                "n (c d) p -> n c p d",
                c=c1,
                d=k**2,
            ),
            BasicFCLayer(
                k**2,
                c2,
                act=act,
                norm=norm,
                drop=drop,
                pre_norm=False,
            ),
        )

    def forward(self, x):
        return self.block(x)


class ConvMpPatch(nn.Module):
    def __init__(
        self,
        c1,
        c2,
        k=3,
        s=1,
        act="noop",
        norm="noop",
        mp=True,
        **kwargs,
    ):
        super().__init__()
        self.block = nn.Sequential(
            ConvLayer(c1, c2, k, s, act=act, norm=norm),
            nn.MaxPool2d(
                kernel_size=kwargs.get("mp_k", 3),
                stride=kwargs.get("mp_s", 2),
                padding=kwargs.get("mp_p", 1),
            )
            if mp
            else nn.Identity(),
        )

    def forward(self, x):
        return self.block(x)


class SPT(nn.Module):
    def __init__(self, c1, c2, patch_size=16):
        super().__init__()

        self.shifting = Shifting(patch_size // 2)

        patch_dim = (c1 * 5) * (patch_size**2)

        self.patching = nn.Sequential(
            Rearrange(
                "b c (h p1) (w p2) -> b (h w) (p1 p2 c)", p1=patch_size, p2=patch_size
            ),
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, c2),
        )

    def forward(self, x):
        out = self.shifting(x)
        out = self.patching(out)

        return out


---
src/visionlab/models/blocks/mlps.py
---
from einops import rearrange
from einops.layers.torch import Rearrange
from torch import nn

from ...utils.ml import swap_dims
from .layers import (
    ExpansionFCLayer,
    FlattenLayer,
    ScaledResidual,
)


class MLPTokenMixerBlock(nn.Module):
    def __init__(
        self,
        in_channels,
        num_patches,
        expansion=4,
        dropout=0.0,
        act="gelu",
    ):
        super().__init__()
        self.block = ScaledResidual(
            FlattenLayer(
                norm_dims=in_channels,
                norm="ln",
                norm_order="pre",
            ),
            Rearrange("n p c -> n c p"),
            ExpansionFCLayer(
                num_patches,
                expansion,
                act=act,
                dropout=dropout,
                norm="noop",
            ),
            Rearrange("n c p -> n p c"),
        )

    def forward(self, x):
        return rearrange(
            self.block(x),
            "n (h w) c -> n c h w",
            w=x.shape[-1],
        )


class MLPChannelMixerBlock(nn.Module):
    def __init__(
        self,
        in_channels,
        expansion=0.5,
        dropout=0.0,
        act="gelu",
    ):
        super().__init__()
        self.block = ScaledResidual(
            FlattenLayer(
                norm_dims=in_channels,
                norm="ln",
                norm_order="pre",
            ),
            ExpansionFCLayer(
                in_channels,
                expansion,
                act=act,
                dropout=dropout,
                norm="noop",
            ),
        )

    def forward(self, x):
        return rearrange(
            self.block(x),
            "n (h w) c -> n c h w",
            w=x.shape[-1],
        )


class MLPMixerBlock(nn.Module):
    def __init__(
        self,
        in_channels,
        num_patches,
        tm_expansion=4,
        cm_expansion=0.5,
        dropout=0.0,
        act="gelu",
    ):
        super().__init__()
        self.num_patches = num_patches
        self.token_mixer = MLPTokenMixerBlock(
            in_channels,
            num_patches,
            tm_expansion,
            dropout=dropout,
            act=act,
        )
        self.channel_mixer = MLPChannelMixerBlock(
            in_channels,
            cm_expansion,
            dropout=dropout,
            act=act,
        )

    def forward(self, x):
        return self.channel_mixer(self.token_mixer(x))


class MLPDimMixerBlock(nn.Module):
    def __init__(
        self,
        norm_channels,
        mixing_channels,
        dim=-1,
        expansion=4,
        dropout=0.0,
        act="gelu",
    ):
        super().__init__()
        self.norm = nn.LayerNorm(norm_channels)
        self.dim = dim
        self.block = ExpansionFCLayer(
            mixing_channels,
            expansion,
            act=act,
            dropout=dropout,
            norm="noop",
        )

    def forward(self, x):
        # x: [n, d1, d2, ... in_dims]
        y = self.norm(x)
        y = swap_dims(y, self.dim, -1)
        y = self.block(y)
        y = swap_dims(y, self.dim, -1)
        return x + y


class MDMLPMixerBlock(nn.Module):
    def __init__(
        self,
        in_channels,
        norm_channels,
        patch_num_h,
        patch_num_w,
        expansion=4,
        dropout=0.0,
        act="gelu",
    ):
        super().__init__()
        self.num_patches_h = patch_num_h
        self.num_patches_w = patch_num_w
        l_dims = [2, 3, 1, 4]
        l_mixing_channels = [
            patch_num_h,
            patch_num_w,
            in_channels,
            norm_channels,
        ]
        self.block = nn.Sequential(
            *[
                MLPDimMixerBlock(
                    norm_channels=norm_channels,
                    mixing_channels=l_mixing_channels[i],
                    dim=l_dims[i],
                    expansion=expansion,
                    dropout=dropout,
                    activation=act,
                )
                for i in range(len(l_dims))
            ],
        )

    def forward(self, x):
        # x: [n, c, p, d]
        # p = num_patches, d=patch_sz**2 (i.e. a flattened patch)
        x = rearrange(
            x,
            "n c (h w) d -> n c h w d",
            h=self.num_patches_h,
            w=self.num_patches_w,
        )
        return rearrange(self.block(x), "n c h w d -> n c (h w) d")


---
src/visionlab/models/blocks/heads.py
---
from typing import List

import torch.nn as nn

from .layers import (
    BasicFCLayer,
    Conv1x1Layer,
)


class ConvHead(nn.Module):
    def __init__(
        self,
        c1: int,
        cn: List[int],
        act="relu",
        last_act="noop",
        norm="bn2d",
    ):
        super().__init__()
        prev = c1
        layers = []
        last = len(cn) - 1
        for idx, c_ in enumerate(cn):
            layers.append(
                Conv1x1Layer(
                    prev,
                    c_,
                    act=act if idx != last else last_act,
                    norm=norm if idx != last else "noop",
                )
            )
            prev = c_
        layers.append(nn.Flatten())
        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)


class FCHead(nn.Module):
    def __init__(
        self,
        c1: int,
        cn: List[int],
        act="relu",
        last_act="noop",
        norm="bn1d",
        drop: float = 0.0,
    ):
        super().__init__()
        prev = c1
        layers = []
        last = len(cn) - 1
        for idx, c_ in enumerate(cn):
            layers.append(
                BasicFCLayer(
                    prev,
                    c_,
                    act=act if idx != last else last_act,
                    drop=drop if idx != last else 0.0,
                    norm=norm if idx != last else "noop",
                ),
            )
            prev = c_

        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)


---
src/visionlab/models/blocks/attentions.py
---
import math
from typing import Literal

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange
from einops.layers.torch import Rearrange

from .layers import (
    BasicFCLayer,
    Conv1x1Layer,
    ConvLayer,
)


class MultiScaleSAV2(nn.Module):
    def __init__(
        self,
        c1,
        h,
        k_q=1,
        k_kv=1,
        s_q=1,
        s_kv=1,
        heads=2,
        drop=0.0,
        **kwargs,
    ):
        super().__init__()
        self.heads = heads
        self.h = h
        self.drop = drop
        self.pool_q = nn.Sequential(
            nn.MaxPool2d(k_q, s_q, (k_q - 1) //
                         2) if k_q > 1 else nn.Identity(),
            Rearrange("n c h w -> n (h w) c"),
        )
        self.pool_kv = nn.Sequential(
            nn.MaxPool2d(k_kv, s_kv, (k_kv - 1) //
                         2) if k_kv > 1 else nn.Identity(),
            Rearrange("n c h w -> n (h w) c"),
        )
        self.qkv = nn.Sequential(
            nn.Linear(
                c1,
                c1 * 3,
                bias=kwargs.get("qkv_bias", False),
            ),
            Rearrange(
                "n (h w) (a m d) -> a (n m) d h w",
                m=heads,
                a=3,
                h=h,
            ),
        )

    def forward(self, x):
        n, p, c = x.shape
        q, k, v = self.qkv(x)  # [n*m, c', h, w]
        q = self.pool_q(q)  # [n*m, (h'*w'), c']
        k = self.pool_kv(k)  # [n*m, (h''*w''), c']
        v = self.pool_kv(v)  # [n*m, (h''*w''), c']
        out = F.scaled_dot_product_attention(
            q,
            k,
            v,
            dropout_p=self.drop,
        )  # [n*m, (h'*w'), c']
        return rearrange(out, "(n m) p d -> n p (m d)", m=self.num_heads)


class MultiHeadSA(nn.Module):
    def __init__(
        self,
        c1,
        heads=2,
        drop=0.0,
        **kwargs,
    ):
        super().__init__()
        self.mha = nn.MultiheadAttention(
            c1,
            heads,
            dropout=drop,
            batch_first=True,
            **kwargs,
        )

    def forward(self, x):
        return self.mha(x, x, x)[0]  # [n, p, c]


class ECABlockV2(nn.Module):
    def __init__(self, c1, gamma=2, bias=1):
        super().__init__()
        k = int(abs(math.log(c1, 2) + bias) / gamma)
        k = k if k % 2 else k + 1

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.block = nn.Conv1d(2, 1, k, padding="same")
        self.act = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.avg_pool(x).squeeze(-1).transpose(-1, -2)
        max_out = self.max_pool(x).squeeze(-1).transpose(-1, -2)
        weights = self.block(torch.cat([avg_out, max_out], dim=1))
        weights = self.act(weights).transpose(-1, -2).unsqueeze(-1)
        return x * weights


class ECABlock(nn.Module):
    def __init__(self, c1, gamma=2, bias=1):
        super().__init__()
        k = int(abs(math.log(c1, 2) + bias) / gamma)
        k = k if k % 2 else k + 1

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.block = nn.Conv1d(1, 1, k, padding="same")
        self.act = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.avg_pool(x).squeeze(-1).transpose(-1, -2)
        weights = self.block(avg_out)
        weights = self.act(weights).transpose(-1, -2).unsqueeze(-1)
        return x * weights


class ChannelAttentionBlock(nn.Module):
    def __init__(self, c1, f=0.8, use_conv=True):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        block_cls = Conv1x1Layer if use_conv else BasicFCLayer
        self.block = nn.Sequential(
            block_cls(
                c1,
                math.ceil(f * c1),
                act="relu",
                norm="noop",
            ),
            block_cls(
                math.ceil(f * c1),
                c1,
                act="noop",
                norm="noop",
            ),
        )
        self.act = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.block(self.avg_pool(x))
        max_out = self.block(self.max_pool(x))
        return x * self.act(avg_out + max_out)


class SpatialAttentionBlock(nn.Module):
    def __init__(self):
        super().__init__()
        self.block = ConvLayer(2, 1, act="sigmoid", norm="noop")

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        return x * self.block(torch.cat([avg_out, max_out], dim=1))


class SEBlock(nn.Module):
    def __init__(self, c1, f=0.25, use_conv=True):
        super().__init__()
        self.pool = nn.AdaptiveAvgPool2d(1)

        block_cls = Conv1x1Layer if use_conv else BasicFCLayer
        self.block = nn.Sequential(
            block_cls(c1, math.ceil(f * c1), act="relu", norm="noop"),
            block_cls(math.ceil(f * c1), c1, act="hsigmoid", norm="noop"),
        )

    def forward(self, x):
        weights = self.pool(x)
        weights = self.block(weights).view(x.size(0), -1, 1, 1)
        return x * weights


class CBAMBlock(nn.Module):
    def __init__(
        self,
        c1,
        ca: Literal["eca", "ecav2", "cbam", "se"] = "eca",
        **ca_kwargs,
    ):
        super().__init__()
        if ca == "eca":
            self.ca = ECABlock(c1, **ca_kwargs)
        elif ca == "ecav2":
            self.ca = ECABlockV2(c1, **ca_kwargs)
        elif ca == "cbam":
            self.ca = ChannelAttentionBlock(c1, **ca_kwargs)
        elif ca == "se":
            self.ca = SEBlock(c1, **ca_kwargs)
        else:
            raise ValueError(f"unknown channel attn type: {ca}")

        self.sa = SpatialAttentionBlock()

    def forward(self, x):
        x = self.ca(x)
        return self.sa(x)


---
src/visionlab/models/blocks/transformers.py
---
import torch.nn as nn
from einops.layers.torch import Rearrange

from ..norm_factory import Norms
from .attentions import MultiHeadSA, MultiScaleSAV2
from .layers import (
    ApplyNorm,
    ExpansionFCLayer,
    ScaledResidual,
    SoftSplit,
    UnflattenLayer,
)


class T2TBlock(nn.Sequential):
    def __init__(
        self,
        c1,
        c2,
        h,
        f=1,
        k=3,
        s=1,
        heads=1,
        act="gelu",
        drop=0.0,
    ):
        super().__init__(
            TransformerEncoder(c1, c1, f, heads, act, drop),
            UnflattenLayer(h=h),
            SoftSplit(c1, c2, k=k, s=s),
        )


class TransformerEncoderMultiScale(nn.Module):
    def __init__(
        self,
        c1,
        c2,
        h,
        f=2,
        k_q=1,
        k_kv=1,
        s_q=1,
        s_kv=1,
        heads=2,
        act="gelu",
        drop=0.0,
        pre_norm=True,
    ):
        super().__init__()
        msa = MultiScaleSAV2(
            c1,
            h,
            k_q,
            k_kv,
            s_q,
            s_kv,
            heads,
            drop,
        )
        msa_shortcut = nn.Sequential(
            Rearrange("n (h w) c -> n c h w", h=h),
            msa.pool_q,
        )

        mlp_shortcut = nn.Sequential(
            Rearrange("n p c -> n c p"),
            nn.Conv1d(c1, c2, 1),
            Rearrange("n c p -> n p c"),
        )

        self.block = nn.Sequential(
            ScaledResidual(
                ApplyNorm(Norms.get("ln", c1), msa, pre_norm),
                shortcut=msa_shortcut,
            ),
            ScaledResidual(
                ExpansionFCLayer(c1, c2, f, act, "ln", drop, pre_norm),
                shortcut=mlp_shortcut,
            ),
        )

    def forward(self, x):
        return self.block(x)


class TransformerEncoder(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        f=2,
        heads=2,
        act="gelu",
        drop=0.0,
        pre_norm=True,
    ):
        super().__init__()
        c2 = c2 or c1
        mlp_shortcut = nn.Sequential(
            Rearrange("n p c -> n c p"),
            nn.Conv1d(c1, c2, 1),
            Rearrange("n c p -> n p c"),
        )
        self.block = nn.Sequential(
            ScaledResidual(
                ApplyNorm(
                    Norms.get("ln", c1),
                    MultiHeadSA(c1, heads, drop),
                    pre_norm,
                )
            ),
            ScaledResidual(
                ExpansionFCLayer(c1, c2, f, act, "ln", drop, pre_norm),
                shortcut=mlp_shortcut,
            ),
        )

    def forward(self, x):
        return self.block(x)  # [n, p, c]


---
src/visionlab/models/blocks/convs.py
---
import torch
import torch.nn as nn

from ...utils.ml import make_divisible
from ..act_factory import Acts
from ..attn_factory import Attentions
from .layers import (
    BottleneckLayer,
    Conv1x1Layer,
    ConvLayer,
    DenseShortcutLayer,
    DWConvLayer,
    DWLightBottleneckLayer,
    DWSepShortcutLayer,
    GhostLayer,
    LightBottleneckLayer,
    ScaledResidual,
    Shortcut3x3Layer,
    ShortcutLayer,
)


class MobileNetBlock(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        f=2,
        k=3,
        s=1,
        act="relu",
        norm="bn2d",
        attn="noop",
    ):
        super().__init__()
        c2 = c2 or c1
        self.block = nn.Sequential(
            ScaledResidual(
                DWLightBottleneckLayer(c1, c2, f, k, s, act, norm),
                Attentions.get(attn, c2),
                shortcut=ShortcutLayer(c1, c2, s),
                apply_shortcut=(s == 1 and c1 == c2),
            ),
            Acts.get(act),
        )

    def forward(self, x):
        return self.block(x)


class GhostBottleneckBlock(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        f=3,
        k=3,
        s=1,
        act="relu",
        norm="bn2d",
        attn="noop",
    ):
        super().__init__()
        c2 = c2 or c1
        c_ = make_divisible(f * c1, 4)
        downsample = (
            nn.Identity()
            if s == 1
            else DWConvLayer(
                c_,
                c_,
                k,
                s,
                act="noop",
            )
        )
        self.block = ScaledResidual(
            GhostLayer(c1, c_, k=k, act=act, norm=norm),
            downsample,
            Attentions.get(attn, c_),
            GhostLayer(c_, c2, k=k, act="noop", norm=norm),
            shortcut=DWSepShortcutLayer(c1, c2, k, s, act, norm),
        )

    def forward(self, x):
        return self.block(x)


class StackedDenseBlock(nn.Module):
    def __init__(
        self,
        n,
        c1,
        f=32,
        act="relu",
        norm="bn2d",
    ):
        super().__init__()
        layers = []
        for _ in range(n):
            layers += [DenseShortcutLayer(c1, f, act=act, norm=norm)]
            c1 += f
        self.block = nn.Sequential(*layers)
        self.c2 = c1

    def forward(self, x):
        return self.block(x)


class LightResNetBlock(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        k=3,
        s=1,
        act="relu",
        norm="bn2d",
        attn="noop",
    ):
        super().__init__()
        c2 = c2 or c1
        self.block = nn.Sequential(
            ScaledResidual(
                nn.MaxPool2d(2, s) if s > 1 else nn.Identity(),
                BottleneckLayer(c1, c2, k, 1, act, norm),
                Attentions.get(attn, c2),
                shortcut=ShortcutLayer(c1, c2, s),
            ),
        )

    def forward(self, x):
        return self.block(x)


class ResNetBlock(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        k=3,
        s=1,
        act="relu",
        norm="bn2d",
        attn="noop",
    ):
        super().__init__()
        c2 = c2 or c1

        self.block = nn.Sequential(
            ScaledResidual(
                BottleneckLayer(c1, c2, k, s, act, norm),
                Attentions.get(attn, c2),
                shortcut=Shortcut3x3Layer(c1, c2, s),
            ),
            Acts.get(act),
        )

    def forward(self, x):
        return self.block(x)


class ResNeXtBlock(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        f=0.5,
        k=3,
        s=1,
        g=1,
        act="relu",
        norm="bn2d",
        attn="noop",
    ):
        super().__init__()
        c2 = c2 or c1

        self.block = nn.Sequential(
            ScaledResidual(
                LightBottleneckLayer(c1, c2, f, k, s, g, act, norm),
                Attentions.get(attn, c2),
                shortcut=ShortcutLayer(c1, c2, s),
            ),
            Acts.get(act),
        )

    def forward(self, x):
        return self.block(x)


class ConvMixerBlock(nn.Module):
    def __init__(
        self,
        c1,
        k=7,
        s=1,
        act="gelu",
        norm="bn2d",
    ):
        super().__init__()
        self.block = nn.Sequential(
            ScaledResidual(
                ConvLayer(
                    c1,
                    c1,
                    k,
                    s,
                    g=c1,
                    act=act,
                    norm=norm,
                ),
                ShortcutLayer(c1, c1, s),
            ),
            Conv1x1Layer(c1, c1, act=act, norm=norm),
        )

    def forward(self, x):
        return self.block(x)


class ConvTokenMixerBlock(nn.Module):
    def __init__(
        self,
        c1,
        seq,
        f=4,
        act="gelu",
        drop=0.0,
    ):
        super().__init__()
        self.block = ScaledResidual(
            nn.LayerNorm(c1),
            nn.Conv1d(seq, int(seq * f), 1),
            nn.Dropout(drop),
            nn.Conv1d(int(seq * f), seq, 1),
            nn.Dropout(drop),
            Acts.get(act),
        )

    def forward(self, x):
        return self.block(x)  # [n, p, c]


class InceptionBlock(nn.Module):
    def __init__(
        self,
        c1,
        cn,
        fn,
        kn,
        act="relu",
        norm="bn2d",
        last_conv1x1=False,
    ):
        """
        l_red_channels: list of reduction channels
        l_out_channels: list of output channels
        l_kernel_sizes: list of kernel sizes
        """
        super().__init__()
        self.branches = nn.ModuleList()
        for i, (f, c2, k) in enumerate(zip(fn, cn, kn)):
            if k == 1:
                layers = (
                    []
                    if f is None
                    else [
                        nn.MaxPool2d(3, 1, 1),
                    ]
                )
                layers.append(Conv1x1Layer(c1, c2, act=act))
                branch = nn.Sequential(*layers)
            else:
                branch = LightBottleneckLayer(
                    c1,
                    c2,
                    f,
                    k,
                    act=act,
                    norm=norm,
                )
            self.branches.append(branch)

    def forward(self, x):
        return torch.cat([branch(x) for branch in self.branches], dim=1)


---
src/visionlab/models/nets/yolo.py
---



---
src/visionlab/models/nets/lenet.py
---
import torch
import torch.nn as nn
from einops.layers.torch import Rearrange
from visionlab.models.blocks.attentions import (
    Attentions,
    MultiHead_SA,
)
from visionlab.models.blocks.composites import (
    ConvMixerBlock,
    DenseNetBlock,
    InceptionBlock,
    MLPMixerBlock,
    ResNetBlock,
)
from visionlab.models.blocks.core import (
    ConvLayer,
)
from visionlab.utils.registry import load_obj
from omegaconf import DictConfig


class LeNet5(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()

        self.layer1 = nn.Sequential(
            ConvLayer(3, 6, kernel_size=5, padding=0),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.layer2 = nn.Sequential(
            ConvLayer(6, 16, kernel_size=5, padding=0),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        self.head = load_obj(config.model.head.class_name)(
            in_features=16,
            **config.model.head.params,
        )

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        return self.head(x)


class LeNetV2(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()

        self.stem = nn.Sequential(
            ConvLayer(3, 6, kernel_size=5, padding=0),
        )

        self.trunk = nn.Sequential(
            ResNetBlock(6, 12, stride=2),
            InceptionBlock(
                12,
                [6, 2],
                [8, 4],
                [3, 5],
            ),
        )

        self.head = load_obj(config.model.head.class_name)(
            in_features=12,
            **config.model.head.params,
        )

    def forward(self, x):
        x = self.stem(x)
        x = self.trunk(x)
        return self.head(x)


class LeNetV3(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()

        self.stem = nn.Sequential(
            ConvLayer(3, 6, kernel_size=5, padding=0),
        )

        self.trunk = nn.Sequential(
            MultiHead_SA(6),
            ResNetBlock(6, 6, attn=Attentions.ECA()),
            ConvLayer(6, 12, stride=2, padding=1),
            ResNetBlock(12, 12),
            ConvLayer(12, 24, stride=2, padding=1),
        )

        self.head = load_obj(config.model.head.class_name)(
            in_features=24,
            **config.model.head.params,
        )

    def forward(self, x):
        x = self.stem(x)
        x = self.trunk(x)
        return self.head(x)


class LeNetV4(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()

        self.stem = nn.Sequential(
            ConvLayer(3, 6, kernel_size=5, padding=0),
        )

        dense_1 = DenseNetBlock(6, 12)
        dense_2 = DenseNetBlock(dense_1.out_channels, 12)

        self.trunk = nn.Sequential(
            MultiHead_SA(6),
            dense_1,
            nn.MaxPool2d(kernel_size=2, stride=2),
            dense_2,
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        self.neck = load_obj(config.model.neck.class_name)(
            in_channels=dense_2.out_channels,
            **config.model.neck.get("params", {}),
        )

        self.head = load_obj(config.model.head.class_name)(
            in_features=dense_2.out_channels,
            **config.model.head.params,
        )

    def forward(self, x):
        x = self.stem(x)
        x = self.trunk(x)
        return self.head(self.neck(x))


class LeNetV5(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()
        self.stem = load_obj(config.model.stem.class_name)(
            in_channels=3,
            out_channels=128,
            **config.model.stem.get("params", {}),
        )
        self.trunk = nn.Sequential(
            ConvMixerBlock(128, kernel_size=7),
            ConvMixerBlock(128, kernel_size=7),
            ConvMixerBlock(128, kernel_size=7),
            ConvMixerBlock(128, kernel_size=7),
        )
        self.head = load_obj(config.model.head.class_name)(
            in_features=128,
            **config.model.head.params,
        )

    def forward(self, x):
        return self.head(self.trunk(self.stem(x)))


class LeNetV6(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()
        self.stem = load_obj(config.model.stem.class_name)(
            in_channels=3,
            out_channels=128,
            **config.model.stem.get("params", {}),
        )
        patch_size = config.model.stem.params.get("patch_size", 1)
        num_patches = (config.model.input_size // patch_size) ** 2
        self.trunk = nn.Sequential(
            MLPMixerBlock(128, num_patches, 2, 0.5),
            MLPMixerBlock(128, num_patches, 2, 0.5),
            MLPMixerBlock(128, num_patches, 2, 0.5),
            MLPMixerBlock(128, num_patches, 2, 0.5),
            MLPMixerBlock(128, num_patches, 2, 0.5),
            MLPMixerBlock(128, num_patches, 2, 0.5),
        )

        self.neck = load_obj(config.model.neck.class_name)(
            in_channels=128,
            **config.model.neck.get("params", {}),
        )

        self.head = load_obj(config.model.head.class_name)(
            in_features=128,
            **config.model.head.get("params", {}),
        )

    def forward(self, x):
        return self.head(self.neck(self.trunk(self.stem(x))))


---
src/visionlab/models/nets/mlpmixer.py
---
import torch.nn as nn
from einops.layers.torch import Reduce
from visionlab.models.blocks.composites import (
    MDMLPMixerBlock,
)
from visionlab.utils.registry import load_obj
from omegaconf import DictConfig


class MDMLPTiny(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()
        n_layers = 6
        embed_sz = 64

        self.stem = load_obj(config.model.stem.class_name)(
            in_channels=3,
            out_channels=embed_sz,
            **config.model.stem.get("params", {}),
        )

        patch_num_h = self.stem.patch_num_h
        patch_num_w = self.stem.patch_num_w

        self.trunk = nn.Sequential(
            *[
                MDMLPMixerBlock(
                    in_channels=3,
                    norm_channels=embed_sz,
                    dropout=0.2,
                    patch_num_h=patch_num_h,
                    patch_num_w=patch_num_w,
                )
                for _ in range(n_layers)
            ],
        )

        self.neck = Reduce("n c p d -> n d", "mean")

        self.head = load_obj(config.model.head.class_name)(
            in_features=embed_sz,
            **config.model.head.get("params", {}),
        )

    def forward(self, x):
        return self.head(self.neck(self.trunk(self.stem(x))))


---
src/visionlab/models/nets/convtiny.py
---
import torch.nn as nn
from visionlab.models.blocks.core import (
    ConvLayer,
    ScaledResidual,
)
from visionlab.utils.ml import init_linear
from visionlab.utils.registry import load_obj
from omegaconf import DictConfig


class Block(ScaledResidual):
    def __init__(self, channels, kernel_size=3, stride=1, mult=4):
        mid_channels = channels * mult
        kernel_size = kernel_size + stride - 1
        super().__init__(
            ConvLayer(
                channels,
                mid_channels,
                kernel_size,
                stride=stride,
                groups=channels,
                padding=(kernel_size - 1) // 2,
            ),
            ConvLayer(
                mid_channels,
                channels,
                1,
            ),
            shortcut=nn.AvgPool2d(stride) if stride > 1 else None,
        )


class Stage(nn.Sequential):
    def __init__(
        self,
        channels,
        num_blocks,
        kernel_size=3,
        stride=1,
        mult=4,
    ):
        super().__init__(
            Block(channels, kernel_size, stride, mult),
            *[Block(channels, kernel_size, 1, mult)
              for _ in range(num_blocks - 1)],
        )


class StageStack(nn.Sequential):
    def __init__(
        self,
        channels,
        num_blocks,
        strides,
        kernel_size=3,
        mult=4,
    ):
        super().__init__(
            *[
                Stage(channels, num_blocks, kernel_size, stride, mult)
                for stride in strides
            ]
        )


class ConvTiny(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()
        embed_sz = 128
        # inter_channels = 48
        strides = [1, 2, 2, 2]

        self.stem = nn.Conv2d(3, embed_sz, 3, padding=1, bias=False)
        self.trunk = StageStack(embed_sz, 2, strides, 3, 4)
        self.neck = load_obj(config.model.neck.class_name)(
            in_channels=embed_sz,
            **config.model.neck.get("params", {}),
        )

        self.head = load_obj(config.model.head.class_name)(
            in_channels=embed_sz,
            **config.model.head.params,
        )

        self.apply(init_linear)

    def forward(self, x):
        return self.head(self.neck(self.trunk(self.stem(x))))


---
src/visionlab/models/nets/t2t.py
---
import math

import numpy as np
import torch
from einops import rearrange, repeat
from einops.layers.torch import Rearrange
from torch import nn


class Residual(nn.Module):
    def __init__(self, *layers, shortcut=None):
        super().__init__()
        self.shortcut = nn.Identity() if shortcut is None else shortcut
        self.residual = nn.Sequential(*layers)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return self.shortcut(x) + self.gamma * self.residual(x)


class SelfAttention(nn.Module):
    def __init__(self, dim, head_dim, heads=8, p_drop=0.0):
        super().__init__()
        inner_dim = head_dim * heads
        self.head_shape = (heads, head_dim)
        self.scale = head_dim**-0.5

        self.to_keys = nn.Linear(dim, inner_dim)
        self.to_queries = nn.Linear(dim, inner_dim)
        self.to_values = nn.Linear(dim, inner_dim)
        self.unifyheads = nn.Linear(inner_dim, dim)

        self.drop = nn.Dropout(p_drop)

    def forward(self, x):
        q_shape = x.shape[:-1] + self.head_shape

        keys = (
            self.to_keys(x).view(q_shape).transpose(1, 2)
        )  # move head forward to the batch dim
        queries = self.to_queries(x).view(q_shape).transpose(1, 2)
        values = self.to_values(x).view(q_shape).transpose(1, 2)

        att = queries @ keys.transpose(-2, -1)
        att = (att * self.scale).softmax(dim=-1)

        out = att @ values
        out = out.transpose(1, 2).contiguous().flatten(2)  # move head back
        out = self.unifyheads(out)
        out = self.drop(out)
        return out


class FeedForward(nn.Sequential):
    def __init__(self, dim, mlp_mult=4, p_drop=0.0):
        hidden_dim = dim * mlp_mult
        super().__init__(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(p_drop),
        )


class TransformerBlock(nn.Sequential):
    def __init__(self, dim, head_dim, heads, mlp_mult=4, p_drop=0.0):
        super().__init__(
            Residual(nn.LayerNorm(dim), SelfAttention(
                dim, head_dim, heads, p_drop)),
            Residual(nn.LayerNorm(dim), FeedForward(
                dim, mlp_mult, p_drop=p_drop)),
        )


class SoftSplit(nn.Module):
    def __init__(self, in_channels, dim, kernel_size=3, stride=2):
        super().__init__()
        padding = (kernel_size - 1) // 2
        self.unfold = nn.Unfold(kernel_size, stride=stride, padding=padding)
        self.project = nn.Linear(in_channels * kernel_size**2, dim)

    def forward(self, x):
        out = self.unfold(x).transpose(1, 2)
        out = self.project(out)
        return out


class Reshape(nn.Module):
    def __init__(self, shape):
        super().__init__()
        self.shape = shape

    def forward(self, x):
        out = x.transpose(1, 2).unflatten(2, self.shape)
        return out


class T2TBlock(nn.Sequential):
    def __init__(
        self,
        image_size,
        token_dim,
        embed_dim,
        heads=1,
        mlp_mult=1,
        stride=2,
        p_drop=0.0,
    ):
        super().__init__(
            TransformerBlock(token_dim, token_dim // heads,
                             heads, mlp_mult, p_drop),
            Reshape((image_size, image_size)),
            SoftSplit(token_dim, embed_dim, stride=stride),
        )


class T2TModule(nn.Sequential):
    def __init__(
        self, in_channels, image_size, strides, token_dim, embed_dim, p_drop=0.0
    ):
        stride = strides[0]
        layers = [SoftSplit(in_channels, token_dim, stride=stride)]
        image_size = image_size // stride

        for stride in strides[1:-1]:
            layers.append(
                T2TBlock(image_size, token_dim, token_dim,
                         stride=stride, p_drop=p_drop)
            )
            image_size = image_size // stride

        stride = strides[-1]
        layers.append(
            T2TBlock(image_size, token_dim, embed_dim,
                     stride=stride, p_drop=p_drop)
        )

        super().__init__(*layers)


class TransformerBackbone(nn.Sequential):
    def __init__(self, dim, head_dim, heads, depth, mlp_mult=4, p_drop=0.0):
        layers = [
            TransformerBlock(dim, head_dim, heads, mlp_mult, p_drop)
            for _ in range(depth)
        ]
        super().__init__(*layers)


class Head(nn.Sequential):
    def __init__(self, dim, classes, p_drop=0.0):
        super().__init__(nn.LayerNorm(dim), nn.Dropout(p_drop), nn.Linear(dim, classes))


class TakeFirst(nn.Module):
    def forward(self, x):
        return x[:, 0]


class PositionEmbedding(nn.Module):
    def __init__(self, image_size, dim):
        super().__init__()
        self.pos_embedding = nn.Parameter(torch.zeros(1, image_size**2, dim))
        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))

    def forward(self, x):
        # add positional embedding
        x = x + self.pos_embedding
        # add classification token
        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)
        return x


class T2TViT(nn.Sequential):
    def __init__(
        self,
        cfg,
        strides=[2, 2, 2],
        token_dim=64,
        dim=128,
        head_dim=64,
        heads=4,
        backbone_depth=8,
        mlp_mult=2,
        in_channels=3,
        image_size=32,
        classes=10,
        trans_p_drop=0.2,
        head_p_drop=0.2,
    ):
        reduced_size = image_size // np.prod(strides)
        super().__init__(
            T2TModule(
                in_channels, image_size, strides, token_dim, dim, p_drop=trans_p_drop
            ),
            PositionEmbedding(reduced_size, dim),
            TransformerBackbone(
                dim,
                head_dim,
                heads,
                backbone_depth,
                mlp_mult=mlp_mult,
                p_drop=trans_p_drop,
            ),
            TakeFirst(),
            Head(dim, classes, p_drop=head_p_drop),
        )


---
src/visionlab/models/nets/vit.py
---
import torch.nn as nn
from einops.layers.torch import Rearrange
from visionlab.models.blocks.attentions import (
    LearnablePositionEnc,
)
from visionlab.models.blocks.composites import TransformerEncoder
from visionlab.utils.ml import init_linear
from visionlab.utils.registry import load_obj
from omegaconf import DictConfig


class ViTTiny(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()
        n_layers = 2
        embed_sz = 128
        heads = 2
        c = 3
        h = w = config.model.input_size

        self.stem = load_obj(config.model.stem.class_name)(
            in_channels=c,
            out_channels=embed_sz,
            **config.model.stem.get("params", {}),
        )

        _, _, stem_h, stem_w = self.stem.out_shape(c, h, w)

        self.trunk = nn.Sequential(
            Rearrange("n d h w -> n (h w) d"),
            LearnablePositionEnc(sizes=(stem_h * stem_w, embed_sz)),
            *[
                TransformerEncoder(
                    embed_sz,
                    num_heads=heads,
                    expansion=1,
                    dropout=0.2,
                )
                for _ in range(n_layers)
            ],
        )

        self.neck = load_obj(config.model.neck.class_name)(
            in_channels=embed_sz,
            **config.model.neck.get("params", {}),
        )

        self.head = load_obj(config.model.head.class_name)(
            in_channels=embed_sz,
            **config.model.head.params,
        )

        self.apply(init_linear)

    def forward(self, x):
        return self.head(self.neck(self.trunk(self.stem(x))))


---
src/visionlab/models/nets/convmixer.py
---
import torch.nn as nn
from einops.layers.torch import Rearrange
from visionlab.models.blocks.composites import (
    ConvMixerBlock,
)
from visionlab.utils.registry import load_obj
from omegaconf import DictConfig


class ConvMixerTiny(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()
        n_layers = 8
        embed_sz = 256
        c = 3

        self.stem = load_obj(config.model.stem.class_name)(
            in_channels=c,
            out_channels=embed_sz,
            **config.model.stem.get("params", {}),
        )

        self.trunk = nn.Sequential(
            *[
                ConvMixerBlock(
                    embed_sz,
                    kernel_size=5,
                )
                for _ in range(n_layers)
            ],
            Rearrange("n c h w -> n (h w) c"),
        )

        self.neck = load_obj(config.model.neck.class_name)(
            in_channels=embed_sz,
            **config.model.neck.get("params", {}),
        )

        self.head = load_obj(config.model.head.class_name)(
            in_features=embed_sz,
            **config.model.head.params,
        )

    def forward(self, x):
        return self.head(self.neck(self.trunk(self.stem(x))))


---
src/visionlab/models/nets/base.py
---
import torch
from visionlab.utils.ml import init_linear
from visionlab.utils.registry import load_module
from torch import nn


class ClfModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg
        self.block = load_module(cfg.model)
        self.output_shape = self.block.output_shape
        self.apply(init_linear)

    def forward(self, x):
        return self.block(x)

    def forward_dummy(self):
        c, h, w = [self.cfg.model.ip.get(i) for i in ("c", "h", "w")]
        return self.forward(torch.randn([self.cfg.training.bs, c, h, w]))


---
src/visionlab/models/nets/ssd.py
---


---
src/visionlab/models/backbones/base.py
---
from typing import Any

import torch.nn as nn


class Backbone(nn.Module):
    def __init__(self) -> None:
        super().__init__()

    def forward(self, x: Any) -> Any:
        return x


---
src/visionlab/models/backbones/resnet.py
---
import timm
import torch.nn as nn
from visionlab.models.backbones.base import Backbone
from visionlab.utils.ml import freeze_until


class ResNetBackbone(Backbone):
    out_features: int = None

    def __init__(
        self,
        arch: str,
        pretrained: bool,
        n_layers: int = -2,
        freeze: bool = False,
        freeze_until_layer: str = None,
    ):
        super().__init__()
        if not arch.startswith("resnet"):
            raise ValueError(f"Unsupported ResNet backbone: {arch}")

        model = timm.create_model(arch, pretrained=pretrained)
        self.out_features = model.fc.in_features
        layers = list(model.children())[:n_layers]
        self.net = nn.Sequential(*layers)

        if freeze:
            freeze_until(self.net, freeze_until_layer)

    def forward(self, x):
        return self.net(x)


---
src/visionlab/lit_models/image_clf.py
---
import lightning as L
import structlog
import torch
import torch.nn as nn
from lightning.fabric.utilities.throughput import measure_flops
from visionlab.utils.registry import load_obj
from omegaconf import DictConfig

log = structlog.get_logger()


class LitImageClassifier(L.LightningModule):
    def __init__(self, cfg: DictConfig) -> None:
        super().__init__()
        self.cfg = cfg
        self.model = load_obj(cfg.model.class_name)(cfg)
        self.model.__class__.__name__ = cfg.general.model_name
        self.loss = load_obj(cfg.loss.class_name)(
            **cfg.loss.get("params", {}),
        )
        self.metrics = nn.ModuleDict()
        for metric_name, metric in cfg.metric.items():
            if metric_name.startswith("_"):
                continue
            self.metrics.update(
                {
                    metric_name: load_obj(metric.class_name)(
                        **metric.params,
                    ),
                }
            )
        self.setup(None)

    def setup(self, stage) -> None:
        with torch.device("meta"):
            model = load_obj(self.cfg.model.class_name)(self.cfg)
            self.flops = measure_flops(
                model,
                model.forward_dummy,
            )
        log.info(f"TFLOPs: {self.flops / 1e12}")

    def forward(self, x, *args, **kwargs):
        return self.model(x)

    def predict_step(self, batch, batch_idx):
        image = batch["image"]
        if image.device != self.device:
            image = image.to(self.device)
        return image, self(image).argmax(dim=-1)

    def configure_optimizers(self):
        if self.cfg.scheduler.class_name.endswith(("OneCycleLR",)):
            self.cfg.scheduler.params.total_steps = (
                self.trainer.estimated_stepping_batches
            )
        optimizer = load_obj(self.cfg.optimizer.class_name)(
            self.model.parameters(), **self.cfg.optimizer.params
        )
        scheduler = load_obj(self.cfg.scheduler.class_name)(
            optimizer, **self.cfg.scheduler.params
        )

        return (
            [optimizer],
            [
                {
                    "scheduler": scheduler,
                    "interval": self.cfg.scheduler.step,
                    "monitor": self.cfg.scheduler.monitor,
                }
            ],
        )

    def training_step(self, batch, *args, **kwargs):
        image = batch["image"]
        logits = self(image)

        target = batch["target"]
        loss = self.loss(logits, target)
        self.log(
            "train_loss",
            loss,
            on_step=False,
            on_epoch=True,
            prog_bar=True,
            logger=True,
        )

        for metric in self.metrics:
            score = self.metrics[metric](logits, target)
            self.log(
                f"train_{metric}",
                score,
                on_step=False,
                on_epoch=True,
                prog_bar=True,
                logger=True,
            )

        lr = self.trainer.lr_scheduler_configs[0].scheduler.get_last_lr()[0]
        self.log(
            "lr",
            lr,
            on_step=True,
            prog_bar=True,
            logger=True,
        )

        return loss

    def validation_step(self, batch, *args, **kwargs):
        image = batch["image"]
        logits = self(image)

        target = batch["target"]
        loss = self.loss(logits, target)

        self.log(
            "valid_loss",
            loss,
            on_epoch=True,
            prog_bar=True,
            logger=True,
        )
        for metric in self.metrics:
            score = self.metrics[metric](logits, target)
            self.log(
                f"valid_{metric}",
                score,
                on_epoch=True,
                prog_bar=True,
                logger=True,
            )

    def test_step(self, batch, *args, **kwargs):
        image = batch["image"]
        target = batch["target"]
        logits = self(image)

        for metric in self.metrics:
            score = self.metrics[metric](logits, target)
            self.log(
                f"test_{metric}",
                score,
                on_epoch=True,
                prog_bar=True,
                logger=True,
            )


---
src/visionlab/datasets/image_clf_ds.py
---
from pathlib import Path
from typing import List, Tuple

import albumentations as A
import cv2
import numpy as np
from albumentations.pytorch import ToTensorV2
from torch.utils.data import Dataset


def create_transforms():
    return A.Compose(
        [
            A.Resize(
                width=224,
                height=224,
            ),
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
            ),
            ToTensorV2(),
        ],
    )


class ImageClfDataset(Dataset):
    def __init__(
        self,
        samples: List[Tuple[Path, str]],
        mode: str = "train",
        transform=None,
        classes=None,
        class_to_idx=None,
    ):
        super().__init__()
        self.samples = samples
        self.mode = mode
        self.transform = transform
        self.classes = classes
        self.class_to_idx = class_to_idx

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        image_path, target = self.samples[idx]

        image = cv2.imread(f"{image_path}", cv2.IMREAD_COLOR)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        if image is None:
            raise FileNotFoundError(image_path)

        if self.transform is not None:
            image = self.transform(image=image)["image"]
        return {"image": image, "target": np.array(target).astype("int64")}


---
src/visionlab/datasets/tv_clf_ds.py
---
import cv2
import numpy as np
import torch
from torchvision.datasets import CIFAR10, MNIST, FashionMNIST


class TorchvisionClfDataset:
    map_converters = {
        "gray2rgb": lambda x: cv2.cvtColor(x, cv2.COLOR_GRAY2RGB),
        "tensor2npy": lambda x: x.numpy(),
        "noop": lambda x: x,
    }

    def __init__(self, dataset_name, **kwargs):
        self.dataset_name = dataset_name
        self._dataset = self._get_dataset(**kwargs)
        self.classes = self._dataset.classes
        self.class_to_idx = self._dataset.class_to_idx
        self.converters = []

        first = self._dataset.data[0]
        if isinstance(first, torch.Tensor):
            self.converters.append(self.map_converters["tensor2npy"])

        if first.ndim == 2:
            self.converters.append(self.map_converters["gray2rgb"])

    def _get_dataset(self, **kwargs):
        if self.dataset_name == "CIFAR10":
            return CIFAR10(**kwargs)
        elif self.dataset_name == "MNIST":
            return MNIST(**kwargs)
        elif self.dataset_name == "FashionMNIST":
            return FashionMNIST(**kwargs)
        else:
            raise ValueError(f"Unsupported dataset: {self.dataset_name}")

    def __getitem__(self, idx):
        image, target = self._dataset.data[idx], int(
            self._dataset.targets[idx])

        for func in self.converters:
            image = func(image)

        return {"image": image, "target": np.array(target).astype("int64")}

    def __len__(self):
        return len(self._dataset)


---
src/visionlab/predict.py
---
from pathlib import Path

import hydra
import lightning as L
import structlog
import torch
from omegaconf import DictConfig, OmegaConf

from visionlab.utils.misc import log_useful_info, set_seed
from visionlab.utils.registry import load_obj

log = structlog.get_logger()


def _predict(
    model: L.LightningModule,
    dm: L.LightningDataModule,
    cfg: DictConfig,
):
    dm.prepare_data()
    dm.setup(stage="predict")
    dl = dm.predict_dataloader()

    model.eval()
    with torch.no_grad():
        for idx, inputs in enumerate(dl):
            images, preds = model.predict_step(inputs, idx)
            yield dm.visualize_batch(images, preds)


def _run(cfg: DictConfig) -> None:
    set_seed(cfg.training.seed)
    log.info("**** Running predict func ****")
    root_dir = Path(cfg.general.root_dir).joinpath(
        cfg.general.exp_name,
    )

    model = load_obj(cfg.training.lit_model.class_name).load_from_checkpoint(
        root_dir.joinpath(cfg.predict.checkpoint),
        cfg=cfg,
    )

    dm = load_obj(cfg.datamodule.class_name)(cfg=cfg)
    return _predict(model, dm, cfg)


@hydra.main(
    version_base=None,
    config_path="conf",
    config_name="config",
)
def run(cfg: DictConfig) -> None:
    Path("logs").mkdir(exist_ok=True)
    log.info(OmegaConf.to_yaml(cfg))
    if cfg.general.log_code:
        log_useful_info()
    _run(cfg)


if __name__ == "__main__":
    run()


---
src/visionlab/__init__.py
---
import logging
import sys

import structlog
from structlog.processors import CallsiteParameter

# Structlog configuration
logging.basicConfig(
    format="%(message)s",
    stream=sys.stdout,
    level=logging.INFO,
)

# Disable uvicorn logging
logging.getLogger("uvicorn.error").disabled = True
logging.getLogger("uvicorn.access").disabled = True
logging.getLogger("lightning").propagate = False


structlog.configure(
    processors=[
        structlog.contextvars.merge_contextvars,
        structlog.processors.add_log_level,
        structlog.processors.StackInfoRenderer(),
        structlog.processors.ExceptionRenderer(),
        structlog.dev.set_exc_info,
        structlog.processors.CallsiteParameterAdder(
            [
                CallsiteParameter.FUNC_NAME,
                CallsiteParameter.LINENO,
            ],
        ),
        structlog.processors.TimeStamper(fmt="iso", utc=True),
        structlog.dev.ConsoleRenderer()
        # structlog.processors.JSONRenderer(),
    ],
    logger_factory=structlog.PrintLoggerFactory(),
)


---
src/visionlab/notes.md
---
- All stems receive and return input as `[n, c, h, w]` format.
- All stems have the func: get_shape which does a forward pass and returns shape.
- Stem `expansions` get multiplied with the `out_channels`.
- `expansions` in other blocks like trunk, head, work on `in_channels`.
- T2T arch works well with narrow and deep transformer blocks.
    - `embed_sz=64`, `n_layers=12`
- CCT arch works well with wide and shallow(er) transformer blocks.
    - `embed_sz=128`, `n_layers=4`


---
src/visionlab/train.py
---
from pathlib import Path

import hydra
import lightning as L
import structlog
import torch
from omegaconf import DictConfig, OmegaConf

from visionlab.utils.misc import log_useful_info, set_seed
from visionlab.utils.registry import load_obj

log = structlog.get_logger()
torch.set_float32_matmul_precision("medium")


def _run(cfg: DictConfig) -> None:
    set_seed(cfg.training.seed)
    log.info("**** Running train func ****")

    exp_name = cfg.general.exp_name
    root_dir = Path(cfg.general.root_dir).joinpath(exp_name)

    loggers = []
    if cfg.logging.log:
        for logger in cfg.logging.loggers:
            loggers.append(load_obj(logger.class_name)(**logger.params))

    callbacks = []
    for callback_name, callback in cfg.callback.items():
        if callback_name in ["model_checkpoint", "vis_dls"]:
            dirpath = root_dir.joinpath(callback.params.dirpath)
            callback.params.dirpath = dirpath.as_posix()
        callback_instance = load_obj(callback.class_name)(**callback.params)
        callbacks.append(callback_instance)

    trainer = L.Trainer(
        logger=loggers,
        callbacks=callbacks,
        **cfg.training.trainer_params,
    )
    model = load_obj(cfg.training.lit_model.class_name)(cfg=cfg)
    dm = load_obj(cfg.datamodule.class_name)(cfg=cfg)
    trainer_kwargs = {}
    if cfg.training.get("resume") is not None:
        ckpt_dir = Path(cfg.callback.model_checkpoint.params.dirpath)
        ckpt_path = ckpt_dir.joinpath(cfg.training.resume.checkpoint).as_posix()
        log.info(f"Resuming from ckpt: {ckpt_path}")
        trainer_kwargs["ckpt_path"] = ckpt_path
    trainer.fit(model, dm, **trainer_kwargs)
    trainer.test(model, dm, ckpt_path="best")
    log.info(f"{root_dir = }")


@hydra.main(
    version_base=None,
    config_path="conf",
    config_name="config",
)
def run(cfg: DictConfig) -> None:
    Path("logs").mkdir(exist_ok=True)
    log.info(OmegaConf.to_yaml(cfg))
    if cfg.general.log_code:
        log_useful_info()
    _run(cfg)


if __name__ == "__main__":
    run()


---
src/visionlab/augs/albumentations_aug.py
---
import albumentations as A
from visionlab.utils.registry import load_obj
from omegaconf import DictConfig
from omegaconf.listconfig import ListConfig


def load_augs(cfg: DictConfig) -> A.Compose:
    """
    Load albumentations

    Args:
        cfg:

    Returns:
        compose object
    """
    augs = []
    for a in cfg:
        if a.class_name == "albumentations.OneOf":
            small_augs = []
            for small_aug in a.params:
                # yaml can't contain tuples, so we need to convert manually
                params = {
                    k: (v if type(v) is not ListConfig else tuple(v))
                    for k, v in small_aug.params.items()
                }
                aug = load_obj(small_aug.class_name)(**params)
                small_augs.append(aug)
            aug = load_obj(a.class_name)(small_augs)
            augs.append(aug)

        else:
            params = {
                k: (v if type(v) is not ListConfig else tuple(v))
                for k, v in a.params.items()
            }
            aug = load_obj(a.class_name)(**params)
            augs.append(aug)

    return A.Compose(augs)


---
src/visionlab/datamodules/parser_factory.py
---
from visionlab.datamodules.folder_structs import FolderStructs


class FolderParserFactory:
    @staticmethod
    def get_folder_parser(folder_struct: str):
        return FolderStructs[folder_struct]


---
src/visionlab/datamodules/image_clf_dm.py
---
import lightning as L
from visionlab.augs.albumentations_aug import load_augs
from visionlab.datamodules.parser_factory import FolderParserFactory
from visionlab.utils.registry import load_obj
from omegaconf import DictConfig
from torch.utils.data import DataLoader


class ImageClfDatamodule(L.LightningDataModule):
    def __init__(self, cfg: DictConfig):
        super().__init__()
        self.cfg = cfg
        self.path = cfg.datamodule.path
        self.dataset_cls = load_obj(self.cfg.datamodule.dataset.class_name)
        self.train_augs = load_augs(self.cfg.augmentation.train)
        self.val_augs = load_augs(self.cfg.augmentation.val)

        self.folder_struct = cfg.datamodule.folder_struct.upper()
        self.folder_struct_parser = FolderParserFactory.get_folder_parser(
            self.folder_struct
        ).value(path_src=self.path, **cfg.datamodule.parser_params)

    def prepare_data(self):
        pass

    def setup(self, stage=None):
        class_names = self.folder_struct_parser.class_names
        class_to_idx = self.folder_struct_parser.class_to_idx

        train_samples = self.folder_struct_parser.get_train()
        val_samples = self.folder_struct_parser.get_val()
        test_samples = self.folder_struct_parser.get_test()

        if self.cfg.training.debug:
            train_samples = train_samples[:100]
            val_samples = val_samples[:100]
            test_samples = test_samples[:100]

        if stage == "fit" or stage is None:
            self.train_dataset = self.dataset_cls(
                samples=train_samples,
                mode="train",
                classes=class_names,
                class_to_idx=class_to_idx,
                transform=self.train_augs,
            )

            self.val_dataset = self.dataset_cls(
                samples=val_samples,
                mode="val",
                classes=class_names,
                class_to_idx=class_to_idx,
                transform=self.val_augs,
            )

        if stage in ["test", "predict"] or stage is None:
            self.test_dataset = self.dataset_cls(
                samples=test_samples,
                mode="test",
                classes=class_names,
                class_to_idx=class_to_idx,
                transform=self.val_augs,
            )

    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.cfg.datamodule.batch_size,
            num_workers=self.cfg.datamodule.num_workers,
            pin_memory=self.cfg.datamodule.pin_memory,
            shuffle=True,
            drop_last=True,
        )

    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.cfg.datamodule.batch_size,
            num_workers=self.cfg.datamodule.num_workers,
            pin_memory=self.cfg.datamodule.pin_memory,
            shuffle=False,
            drop_last=False,
        )

    def test_dataloader(self):
        bs = self.cfg.datamodule.test_batch_size if self.cfg.training.debug else 4
        return DataLoader(
            self.test_dataset,
            batch_size=bs,
            num_workers=self.cfg.datamodule.num_workers,
            pin_memory=self.cfg.datamodule.pin_memory,
            shuffle=False,
            drop_last=False,
        )

    def predict_dataloader(self):
        bs = self.cfg.datamodule.test_batch_size if self.cfg.training.debug else 4
        return DataLoader(
            self.test_dataset,
            batch_size=bs,
            num_workers=self.cfg.datamodule.num_workers,
            pin_memory=self.cfg.datamodule.pin_memory,
            shuffle=True,
            drop_last=False,
        )


---
src/visionlab/datamodules/folder_structs.py
---
from enum import Enum

from visionlab.datamodules.parsers.cls_subfolder import ClassSubfolderParser
from visionlab.datamodules.parsers.splits_subfolder import (
    SplitsSubfolderParser,
)


class FolderStructs(Enum):
    CLS_SUBFOLDER = ClassSubfolderParser
    TRAIN_VAL_TEST = SplitsSubfolderParser


---
src/visionlab/datamodules/tv_clf_dm.py
---
import lightning as L
import numpy as np
import structlog
import torch
from visionlab.augs.albumentations_aug import load_augs
from visionlab.utils.registry import load_obj
from visionlab.utils.vis import draw_labels_on_images
from omegaconf import DictConfig
from torch.utils.data import DataLoader, random_split
from torch.utils.data._utils.collate import default_collate

log = structlog.get_logger()


class TorchvisionClfDatamodule(L.LightningDataModule):
    def __init__(self, cfg: DictConfig) -> None:
        super().__init__()
        self.cfg = cfg
        self.path = cfg.datamodule.path
        self.name = cfg.datamodule.dataset.name
        self.splits = cfg.datamodule.splits
        self.dataset_cls = load_obj(cfg.datamodule.dataset.class_name)
        self.train_augs = load_augs(cfg.augmentation.train)
        self.val_augs = load_augs(cfg.augmentation.val)
        self.train_collate_fn = self.get_collate_fn(self.train_augs)
        self.val_collate_fn = self.get_collate_fn(self.val_augs)

    def get_collate_fn(self, transform):
        def collate_fn(batch):
            [
                sample.update(
                    {
                        "image": transform(
                            image=sample["image"],
                        )["image"]
                    }
                )
                for sample in batch
            ]
            return default_collate(batch)

        return collate_fn

    def prepare_data(self):
        self.dataset_cls(
            self.name,
            root=self.path,
            train=True,
            download=True,
        )
        self.dataset_cls(
            self.name,
            root=self.path,
            train=False,
            download=True,
        )

    def setup(self, stage=None):
        if stage == "fit" or stage is None:
            self.train_dataset = self.dataset_cls(
                self.name,
                root=self.path,
                train=True,
            )

            self.val_dataset = self.dataset_cls(
                self.name,
                root=self.path,
                train=False,
            )
            self.classes = self.train_dataset.classes

            if self.cfg.training.debug:
                self.train_dataset, _ = random_split(
                    self.train_dataset,
                    [0.1, 0.9],
                )
                self.val_dataset, _ = random_split(
                    self.val_dataset,
                    [0.1, 0.9],
                )
            log.info(f"train_dataset size: {len(self.train_dataset)}")
            log.info(f"val_dataset size: {len(self.val_dataset)}")

        if stage in ["fit", "test", "predict"] or stage is None:
            self.test_dataset = self.dataset_cls(
                self.name,
                root=self.path,
                train=False,
            )
            self.classes = self.test_dataset.classes

    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.cfg.datamodule.batch_size,
            num_workers=self.cfg.datamodule.num_workers,
            pin_memory=self.cfg.datamodule.pin_memory,
            collate_fn=self.train_collate_fn,
            shuffle=True,
            drop_last=True,
        )

    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.cfg.datamodule.batch_size,
            num_workers=self.cfg.datamodule.num_workers,
            pin_memory=self.cfg.datamodule.pin_memory,
            collate_fn=self.val_collate_fn,
            shuffle=False,
            drop_last=False,
        )

    def test_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_size=self.cfg.datamodule.test_batch_size,
            num_workers=self.cfg.datamodule.num_workers,
            collate_fn=self.val_collate_fn,
            pin_memory=False,
            shuffle=False,
            drop_last=False,
        )

    def predict_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_size=self.cfg.datamodule.test_batch_size,
            num_workers=self.cfg.datamodule.num_workers,
            collate_fn=self.val_collate_fn,
            pin_memory=False,
            shuffle=True,
            drop_last=False,
        )

    def visualize_batch(self, inputs, outputs=None, num_samples=8):
        if outputs is None:
            outputs = inputs["target"]
            inputs = inputs["image"]
        outputs_resolved = np.array(self.classes)[outputs.detach().cpu()]

        std = torch.tensor(self.cfg.datamodule.dataset.std).view(3, 1, 1)
        mean = torch.tensor(self.cfg.datamodule.dataset.mean).view(3, 1, 1)
        images = inputs.detach().cpu() * std + mean

        return draw_labels_on_images(
            images[:num_samples],
            outputs_resolved[:num_samples],
            20,
            size=120,
        )


---
src/visionlab/datamodules/parsers/cls_subfolder.py
---
import random
from pathlib import Path
from typing import List

from visionlab.utils.misc import split_list


class ClassSubfolderParser:
    """
    dataset/
    ├── class1/
    │   ├── image1.jpg
    │   ├── image2.jpg
    │   └── ...
    ├── class2/
    │   ├── image1.jpg
    │   ├── image2.jpg
    │   └── ...
    └── ...
    """

    def __init__(
        self,
        path_src: str,
        train_val_test_split: List[float] = [0.7, 0.2, 0.1],
    ):
        self.path_src = Path(path_src)
        self.train_val_test_split = train_val_test_split
        self.class_names = self._get_class_names()
        self.class_to_idx = {
            class_name: i for i, class_name in enumerate(self.class_names)
        }
        all_samples = self._gather_samples()
        (
            self.train_samples,
            self.val_samples,
            self.test_samples,
        ) = split_list(all_samples, train_val_test_split)

    def _get_class_names(self) -> List[str]:
        return sorted(
            [
                d.name
                for d in self.path_src.iterdir()
                if d.is_dir() and not d.name.startswith(".")
            ]
        )

    def _gather_samples(self):
        samples = []
        for class_name in self.class_names:
            class_path = self.path_src.joinpaht(class_name)
            for image_path in class_path.iterdir():
                if image_path.suffix in [".jpg", ".jpeg", ".png", ".bmp"]:
                    samples.append(
                        (
                            image_path,
                            self.mapping_dict[class_name],
                        )
                    )
        return samples

    def get_train(self, shuffle=True):
        if shuffle:
            random.shuffle(self.train_samples)
        self.train_samples

    def get_val(self):
        return self.val_samples

    def get_test(self):
        return self.test_samples


---
src/visionlab/datamodules/parsers/splits_subfolder.py
---
import random
from pathlib import Path
from typing import List, Tuple


class SplitsSubfolderParser:
    """
    dataset/
    ├── train/
    │   ├── class1/
    │   │   ├── image1.jpg
    │   │   └── ...
    │   ├── class2/
    │   │   ├── image1.jpg
    │   │   └── ...
    │   └── ...
    ├── val/
    │   ├── class1/
    │   │   ├── image1.jpg
    │   │   └── ...
    │   ├── class2/
    │   │   ├── image1.jpg
    │   │   └── ...
    │   └── ...
    └── test/
        ├── class1/
        │   ├── image1.jpg
        │   └── ...
        ├── class2/
        │   ├── image1.jpg
        │   └── ...
        └── ...
    """

    def __init__(self, path_src: str):
        self.path_src = Path(path_src)
        self.class_names = self._get_class_names()
        self.class_to_idx = {
            cls_name: idx for idx, cls_name in enumerate(self.class_names)
        }

    def _get_class_names(self) -> List[str]:
        train_dir = self.path_src.joinpath("train")
        class_names = sorted(
            [
                d.name
                for d in train_dir.iterdir()
                if d.is_dir() and not d.name.startswith(".")
            ]
        )
        return class_names

    def _gather_samples(self, split: str) -> List[Tuple[Path, int]]:
        split_dir = self.path_src.joinpath(split)
        samples = []

        for class_idx, class_name in enumerate(self.class_names):
            class_dir = split_dir.joinpath(class_name)
            for image_path in class_dir.iterdir():
                if image_path.suffix.lower() in [".png", ".jpg", ".jpeg", ".bmp"]:
                    samples.append((image_path, class_idx))

        return samples

    def get_train(self, shuffle=True) -> List[Tuple[Path, int]]:
        train_samples = self._gather_samples("train")
        if shuffle:
            random.shuffle(train_samples)
        return train_samples

    def get_val(self) -> List[Tuple[Path, int]]:
        return self._gather_samples("valid")

    def get_test(self) -> List[Tuple[Path, int]]:
        return self._gather_samples("test")


---
src/visionlab/utils/ml.py
---
from typing import Any, Optional

import torch
from torch import nn


def freeze_until(net: Any, param_name: Optional[str]) -> None:
    """
    Freeze net until param_name

    https://opendatascience.slack.com/archives/CGK4KQBHD/p1588373239292300?thread_ts=1588105223.275700&cid=CGK4KQBHD

    Args:
        net:
        param_name:

    """
    found_name = False
    for name, params in net.named_parameters():
        if name == param_name:
            found_name = True
        params.requires_grad = found_name
    return found_name


def count_parameters(model: Any):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def swap_dims(x, d1, d2):
    # x: torch.tensor, d1 and d2: dims to swap
    dims = list(range(x.dim()))
    dims[d1], dims[d2] = dims[d2], dims[d1]
    return x.permute(dims)


def init_linear(m):
    if isinstance(m, (nn.Linear, nn.Conv2d)):
        nn.init.xavier_normal_(m.weight)
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, nn.BatchNorm2d):
        nn.init.constant_(m.weight, 1.0)
        nn.init.zeros_(m.bias)
    elif isinstance(m, nn.LayerNorm):
        nn.init.constant_(m.weight, 1.0)
        nn.init.zeros_(m.bias)


def compute_output_shape(module, input_shape):
    with torch.no_grad():
        dummy_input = torch.zeros(1, *input_shape)
        output = module(dummy_input)
        return output.shape[1:]


def make_divisible(v, divisor, min_value=None):
    """
    This function is taken from the original tf repo.
    It ensures that all layers have a channel number that is divisible by 8
    It can be seen here:
    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
    """
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    # Make sure that round down does not go down by more than 10%.
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v


---
src/visionlab/utils/vis.py
---
import torch
from torchvision.transforms.functional import to_pil_image
from torchvision.transforms.v2 import Resize
from torchvision.utils import draw_bounding_boxes, make_grid


def draw_labels_on_images(images, labels, label_h=30, size=None):
    labeled_images = []
    for image, label in zip(images, labels):
        if size is not None:
            image = Resize(size=size)(image)
        text_label = f"{label}"

        black_bg = torch.zeros(
            (3, label_h, image.shape[2]),
            dtype=torch.uint8,
        )

        labeled_bg = draw_bounding_boxes(
            image=black_bg,
            boxes=torch.tensor([[0, 0, black_bg.shape[-1], label_h]]),
            labels=[text_label],
            colors=["white"],
            font="../assets/Ubuntu-R.ttf",
            fill=False,
            width=2,
            font_size=label_h * 0.7,
        )

        labeled_image = torch.cat(
            (labeled_bg, (255 * image).byte()),
            dim=1,
        )
        labeled_images.append(labeled_image)

    return torch.stack(labeled_images, dim=0)


def gridify(visualized, fp=None, scale=None, **kwargs):
    im = to_pil_image(make_grid(visualized, **kwargs))
    if scale is not None:
        og_size = im.size
        size = tuple([int(x * scale) for x in og_size])
        im = im.resize(size)
    if fp is not None:
        im.save(fp)
    else:
        return im


---
src/visionlab/utils/registry.py
---
import importlib
from collections import OrderedDict
from typing import Any

from visionlab.utils.ml import compute_output_shape
from omegaconf import DictConfig
from torch import nn


def load_obj(obj_path: str, namespace: str = "visionlab") -> Any:
    """
    Extract an object from a given path.
    https://github.com/quantumblacklabs/kedro/blob/9809bd7ca0556531fa4a2fc02d5b2dc26cf8fa97/kedro/utils.py
        Args:
            obj_path: Path to an object to be extracted, including the object name.
            namespace: Default namespace.
        Returns:
            Extracted object.
        Raises:
            AttributeError: When the object does not have the given named attribute.
    """
    obj_path_list = obj_path.rsplit(".", 1)
    obj_path = obj_path_list.pop(0) if len(obj_path_list) > 1 else ""
    obj_name = obj_path_list[0]
    try:
        module_obj = importlib.import_module(f"{obj_path}")
    except ModuleNotFoundError:
        module_obj = importlib.import_module(f"{namespace}.{obj_path}")
    if not hasattr(module_obj, obj_name):
        raise AttributeError(
            f"Object `{obj_name}` cannot be loaded from `{obj_path}`.")
    return getattr(module_obj, obj_name)


def load_module(cfg):
    layers = cfg["layers"]
    c, h, w = (cfg["ip"].get(i) for i in ("c", "h", "w"))
    input_shape = (c, h, w)
    custom_modules = "models.blocks"

    module_dict = OrderedDict()
    for layer_name, layer_config in layers.items():
        from_layer, num_repeats, block, block_args = layer_config

        if from_layer == "ip":
            input_shape = (c, h, w)
        else:
            input_shape = module_dict[from_layer].output_shape

        if block.startswith("nn."):
            block_class = getattr(nn, block.split(".", 1)[1])
        else:
            try:
                block_class = load_obj(f"{custom_modules}.{block}")
            except KeyError:
                raise ImportError(f"Block '{block}' not found.")

        pos_args = []
        kwargs = {}
        for arg in block_args:
            if isinstance(arg, DictConfig):
                kwargs.update(arg)
            else:
                pos_args.append(arg)

        module = block_class(*pos_args, **kwargs)
        module.output_shape = compute_output_shape(module, input_shape)

        blocks = [module]
        for _ in range(num_repeats - 1):
            module = block_class(*pos_args, **kwargs)
            module.output_shape = compute_output_shape(module, input_shape)
            blocks.append(module)

        stage = nn.Sequential(*blocks)
        stage.output_shape = module.output_shape
        module_dict[layer_name] = stage
    model = nn.Sequential(module_dict)
    model.output_shape = compute_output_shape(model, (c, h, w))
    return model


---
src/visionlab/utils/config.py
---
import collections
from itertools import product
from typing import Dict, Generator

from omegaconf import DictConfig, OmegaConf


def product_dict(**kwargs: Dict) -> Generator:
    """
    Convert dict with lists in values into lists of all combinations

    This is necessary to convert config with experiment values
    into format usable by hydra
    Args:
        **kwargs:

    Returns:
        list of lists

    ---
    Example:
        >>> list_dict = {'a': [1, 2], 'b': [2, 3]}
        >>> list(product_dict(**list_dict))
        >>> [['a=1', 'b=2'], ['a=1', 'b=3'], ['a=2', 'b=2'], ['a=2', 'b=3']]

    """
    keys = kwargs.keys()
    vals = kwargs.values()
    for instance in product(*vals):
        zip_list = list(zip(keys, instance))
        yield [f"{i}={j}" for i, j in zip_list]


def config_to_hydra_dict(cfg: DictConfig) -> Dict:
    """
    Convert config into dict with lists of values, where key is full name of parameter

    This fuction is used to get key names which can be used in hydra.

    Args:
        cfg:

    Returns:
        converted dict

    """
    experiment_dict = {}
    for k, v in cfg.items():
        for k1, v1 in v.items():
            experiment_dict[f"{k!r}.{k1!r}"] = v1

    return experiment_dict


def flatten_omegaconf(d, sep="_"):
    d = OmegaConf.to_container(d)

    obj = collections.OrderedDict()

    def recurse(t, parent_key=""):
        if isinstance(t, list):
            for i, _ in enumerate(t):
                recurse(t[i], parent_key + sep + str(i) if parent_key else str(i))
        elif isinstance(t, dict):
            for k, v in t.items():
                recurse(v, parent_key + sep + k if parent_key else k)
        else:
            obj[parent_key] = t

    recurse(d)
    obj = {k: v for k, v in obj.items() if isinstance(v, (int, float))}

    return obj


---
src/visionlab/utils/misc.py
---
import os
import random
from typing import Any, List

import hydra
import numpy as np
import structlog
import torch

log = structlog.get_logger()


def set_seed(seed: int = 666, precision: int = 10) -> None:
    np.random.seed(seed)
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.set_printoptions(precision=precision)


def log_useful_info() -> None:
    log.info(hydra.utils.get_original_cwd())
    log.info(os.getcwd())


def split_list(data: List[Any], percentages: List[float]) -> List[List[Any]]:
    """
    Splits a list into sublists according to the given percentages.

    Parameters:
        data (List[Any]): The list to be split.
        percentages (List[float]): A list of percentages for splitting.

    Returns:
        List[List[Any]]: A list of sublists split according to the percentages.
    """
    if not sum(percentages) == 1.0:
        raise ValueError("The sum of the percentages must be 1.0")

    data_copy = data[:]
    random.shuffle(data_copy)
    total_len = len(data_copy)

    splits = []
    start = 0
    for pct in percentages:
        end = start + int(pct * total_len)
        splits.append(data_copy[start:end])
        start = end

    return splits


---
src/visionlab/callbacks/vis_dls.py
---
from pathlib import Path

from lightning.pytorch.callbacks import Callback
from visionlab.utils.vis import gridify


class VisualizeDlsCallback(Callback):
    def __init__(
        self,
        num_samples=8,
        dirpath="dls",
        scale=None,
    ):
        super().__init__()
        self.num_samples = num_samples
        self.dirpath = Path(dirpath)
        self.scale = scale

    def on_fit_start(self, trainer, pl_module):
        dm = trainer.datamodule
        train_dls = dm.train_dataloader()
        val_dls = dm.val_dataloader()

        train_batch = next(iter(train_dls))
        val_batch = next(iter(val_dls))

        train_vis = dm.visualize_batch(
            train_batch,
            num_samples=self.num_samples,
        )
        val_vis = dm.visualize_batch(
            val_batch,
            num_samples=self.num_samples,
        )

        self.dirpath.mkdir(parents=True, exist_ok=True)
        gridify(
            train_vis,
            self.dirpath.joinpath("train.jpg"),
            scale=self.scale,
            nrow=4,
        )
        gridify(
            val_vis,
            self.dirpath.joinpath("val.jpg"),
            scale=self.scale,
            nrow=4,
        )


---
src/visionlab/models/act_factory.py
---
from torch import nn

noop = nn.Identity()


class Acts:
    @staticmethod
    def get(act: str, *args, **kwargs):
        if act == "noop":
            return noop
        if act == "relu":
            return nn.ReLU(*args, **kwargs)
        elif act == "gelu":
            return nn.GELU(*args, **kwargs)
        elif act == "silu":
            return nn.SiLU(*args, **kwargs)
        elif act == "sigmoid":
            return nn.Sigmoid(*args, **kwargs)
        elif act == "hsigmoid":
            return nn.Hardsigmoid(*args, **kwargs)
        elif act == "hswish":
            return nn.Hardswish(*args, **kwargs)
        else:
            raise NotImplementedError


---
src/visionlab/models/attn_factory.py
---
from visionlab.models.blocks.attentions import (
    CBAMBlock,
    ChannelAttentionBlock,
    ECABlock,
    ECABlockV2,
    SEBlock,
    SpatialAttentionBlock,
)
from torch import nn

noop = nn.Identity()


class Attentions:
    @staticmethod
    def get(attn: str, *args, **kwargs):
        if attn == "ca":
            return ChannelAttentionBlock(*args, **kwargs)
        elif attn == "sa":
            return SpatialAttentionBlock(*args, **kwargs)
        elif attn == "cbam":
            return CBAMBlock(*args, **kwargs)
        elif attn == "se":
            return SEBlock(*args, **kwargs)
        elif attn == "eca":
            return ECABlock(*args, **kwargs)
        elif attn == "ecav2":
            return ECABlockV2(*args, **kwargs)
        else:
            return noop


---
src/visionlab/models/norm_factory.py
---
from torch import nn

noop = nn.Identity()


class Norms:
    @staticmethod
    def get(norm: str, *args, **kwargs):
        if norm == "noop":
            return noop
        if norm == "bn1d":
            return nn.BatchNorm1d(*args, **kwargs)
        elif norm == "bn2d":
            return nn.BatchNorm2d(*args, **kwargs)
        elif norm == "ln":
            return nn.LayerNorm(*args, **kwargs)
        else:
            raise NotImplementedError


---
src/visionlab/models/blocks/necks.py
---
from einops.layers.torch import Reduce
from torch import einsum, nn

from .layers import (
    Conv1x1Layer,
    NormAct,
)


class AvgPool(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        act="relu",
        norm="bn2d",
        drop=0.0,
        flatten=True,
    ):
        super().__init__()
        c2 = c2 or c1
        self.block = nn.Sequential(
            NormAct(c1, norm, act="noop"),
            nn.AdaptiveAvgPool2d(1),
            Conv1x1Layer(
                c1,
                c2,
                act=act,
                norm="noop",
            )
            if c2 != c1
            else nn.Identity(),
            nn.Flatten(1) if flatten else nn.Identity(),
            nn.Dropout(drop),
        )

    def forward(self, x):
        # [x: n, c, h, w]
        return self.block(x)  # [n, d, 1, 1] or [n, d]


class PatchNorm(nn.Module):
    def __init__(self, c1):
        super().__init__()

        self.block = nn.Sequential(
            nn.LayerNorm(c1),
            Reduce("n p c -> n c", "mean"),
        )

    def forward(self, x):
        return self.block(x)


class SequencePooling(nn.Module):
    def __init__(self, c1):
        super().__init__()
        self.attn_pool = nn.Sequential(
            nn.LayerNorm(c1),
            nn.Linear(c1, 1),
        )

    def forward(self, x):
        # x: [n, p, c]
        attn_weights = self.attn_pool(x).squeeze(-1)  # [n, p]
        return einsum(
            "n p, n p c -> n c",
            attn_weights.softmax(dim=1),
            x,
        )


---
src/visionlab/models/blocks/layers.py
---
import math

import torch
import torch.nn.functional as F
from einops import rearrange
from einops.layers.torch import Rearrange
from visionlab.models.act_factory import Acts
from visionlab.models.norm_factory import Norms
from torch import nn


class Shifting(nn.Module):
    def __init__(self, shift):
        super().__init__()
        self.shift = shift

    def forward(self, x):
        x_pad = F.pad(x, (self.shift, self.shift, self.shift, self.shift))
        x_lu = x_pad[:, :, : -self.shift * 2, : -self.shift * 2]
        x_ru = x_pad[:, :, : -self.shift * 2, self.shift * 2:]
        x_lb = x_pad[:, :, self.shift * 2:, : -self.shift * 2]
        x_rb = x_pad[:, :, self.shift * 2:, self.shift * 2:]
        x_cat = torch.cat([x, x_lu, x_ru, x_lb, x_rb], dim=1)
        return x_cat


class NormAct(nn.Module):
    def __init__(self, c1, norm="bn2d", act="relu"):
        super().__init__()
        self.block = nn.Sequential(
            Norms.get(norm, c1),
            Acts.get(act),
        )

    def forward(self, x):
        return self.block(x)


class ApplyNorm(nn.Module):
    def __init__(self, norm, fn, pre=True):
        super().__init__()
        layers = [norm, fn] if pre else [fn, norm]
        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)


class BasicFCLayer(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        act="relu",
        norm="ln",
        drop=0.0,
        pre_norm=True,
    ):
        super().__init__()
        c2 = c2 or c1
        fc_layers = [
            nn.Linear(c1, c2),
            Acts.get(act),
            nn.Dropout(drop),
        ]
        self.norm = norm

        norm_dims = c1 if pre_norm else c2
        self.block = ApplyNorm(
            Norms.get(norm, norm_dims),
            nn.Sequential(*fc_layers),
            pre_norm,
        )

    def forward(self, x):
        return self.block(x)


class ExpansionFCLayer(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        f=4,
        act="relu",
        norm="ln",
        drop=0.0,
        pre_norm=True,
    ):
        super().__init__()
        c2 = c2 or c1
        self.block = ApplyNorm(
            Norms.get(norm, c1 if pre_norm else c2),
            nn.Sequential(
                BasicFCLayer(
                    c1,
                    math.ceil(c1 * f),
                    act=act,
                    drop=0.0,
                    norm="noop",
                ),
                BasicFCLayer(
                    math.ceil(c1 * f),
                    c2,
                    act="noop",
                    drop=drop,
                    norm="noop",
                ),
            ),
            pre_norm,
        )

    def forward(self, x):
        return self.block(x)


class SoftSplit(nn.Module):
    def __init__(self, c1, c2=None, k=3, s=1, proj=True):
        super().__init__()
        c2 = c2 or c1
        self.block = nn.Sequential(
            nn.Unfold(kernel_size=k, stride=s, padding=(k - 1) // 2),
            Rearrange("n c p -> n p c"),
            nn.Linear(c1 * k**2, c2) if proj else nn.Identity(),
        )

    def forward(self, x):
        # ip: [n, c, w, h], op = [n, p, c]
        # p = (h - k + 2 * padding) / s + 1)**2
        return self.block(x)


class FlattenLayer(nn.Module):
    def __init__(self, c1=None, norm="noop", pre_norm=True):
        super().__init__()
        self.block = ApplyNorm(
            Norms.get(norm, c1),
            nn.Identity(),
            pre_norm,
        )

    def forward(self, x):
        if len(x.shape) == 4:  # Input shape: [n, c, h, w]
            return self.block(rearrange(x, "n c h w -> n (h w) c"))
        elif len(x.shape) == 3:  # Input shape: [n, p, c]
            return self.block(x)
        else:
            raise ValueError(f"Unsupported input shape: {x.shape}")


class UnflattenLayer(nn.Module):
    def __init__(self, h=0):
        super().__init__()
        self.h = h

    def forward(self, x):
        if len(x.shape) == 4:  # Input shape: [n, c, h, w]
            return x
        elif len(x.shape) == 3:  # [n, p, c]
            p = x.shape[1]
            self.h = self.h or int(p**0.5)
            return rearrange(x, "n (h w) c -> n c h w", h=self.h)
        else:
            raise ValueError(f"Unsupported input shape: {x.shape}")


class ConvLayer(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        k=3,
        s=1,
        p="same",
        g=1,
        act="relu",
        norm="bn2d",
        pre_normact=False,
        **kwargs,
    ):
        super().__init__()
        c2 = c2 or c1
        if s > 1 and p == "same":
            p = (k - 1) // 2
        layers = [
            nn.Conv2d(
                c1,
                c2,
                kernel_size=k,
                padding=p,
                stride=s,
                groups=g,
                **kwargs,
            ),
            NormAct(c1 if pre_normact else c2, norm, act),
        ]
        if pre_normact:
            layers.reverse()
        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)


class DWConvLayer(ConvLayer):
    def __init__(self, c1, c2=None, k=3, s=1, **kwargs):
        assert c2 % c1 == 0, f"c2 {c2} must be divisible by c1 {c1}"
        super().__init__(c1, c2, k=k, s=s, g=c1, **kwargs)


class DWSepConvLayer(nn.Sequential):
    def __init__(self, c1, c2=None, k=3, s=1, pw_act="noop", **kwargs):
        c2 = c2 or c1
        super().__init__(
            DWConvLayer(c1, c1, k, s, **kwargs),
            Conv1x1Layer(c1, c2, act=pw_act),
        )


class Conv1x1Layer(ConvLayer):
    def __init__(self, c1, c2=None, s=1, **kwargs):
        super().__init__(c1, c2, k=1, s=s, **kwargs)


class DownsampleConvLayer(ConvLayer):
    def __init__(self, c1, c2=None, k=3, s=2, **kwargs):
        super().__init__(c1, c2, k=k, s=s, **kwargs)


class DownsamplePoolLayer(nn.Module):
    def __init__(self, c1, c2=None, pool="avg", **kwargs):
        super().__init__()
        self.conv = Conv1x1Layer(c1, c2, **kwargs)
        if pool == "avg":
            self.pool = nn.AvgPool2d(k=2, s=2)
        elif pool == "max":
            self.pool = nn.MaxPool2d(k=2, s=2)
        else:
            raise ValueError(f"Unsupported pool type: {pool}")

    def forward(self, x):
        return self.pool(self.conv(x))


class ShortcutLayer(nn.Module):
    def __init__(self, c1, c2=None, s=1):
        super().__init__()
        c2 = c2 or c1
        if c1 == c2 and s == 1:
            self.shortcut = nn.Identity()
        else:
            self.shortcut = Conv1x1Layer(c1, c2, s=s, act="noop")

    def forward(self, x):
        return self.shortcut(x)


class LightShortcutLayer(nn.Module):
    def __init__(self, c1, c2=None, k=3, s=1, g=1):
        super().__init__()
        c2 = c2 or c1
        if c1 == c2 and s == 1:
            self.shortcut = nn.Identity()
        else:
            self.shortcut = nn.Sequential(
                nn.AvgPool2d(k, stride=s, padding=1),
                Conv1x1Layer(c1, c2, g=g, act="noop"),
            )

    def forward(self, x):
        return self.shortcut(x)


class Shortcut3x3Layer(nn.Module):
    def __init__(self, c1, c2=None, s=1):
        super().__init__()
        c2 = c2 or c1
        if c1 == c2 and s == 1:
            self.shortcut = nn.Identity()
        else:
            self.shortcut = ConvLayer(c1, c2, s=s, act="noop")

    def forward(self, x):
        return self.shortcut(x)


class GhostLayer(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        k=3,
        s=1,
        act="relu",
        norm="bn2d",
    ):
        super().__init__()
        c2 = c2 or c1
        assert c2 % 2 == 0, "Output channels should be even"
        c_ = c2 // 2
        self.block_1 = Conv1x1Layer(c1, c_, act=act, norm=norm)
        self.block_2 = DWConvLayer(c_, c_, k=k, s=s, act=act, norm=norm)

    def forward(self, x):
        x1 = self.block_1(x)
        return torch.cat([x1, self.block_2(x1)], dim=1)


class DWSepShortcutLayer(nn.Module):
    def __init__(self, c1, c2, k=3, s=1, act="relu", norm="bn2d"):
        super().__init__()
        if c1 == c2 and s == 1:
            self.shortcut = nn.Identity()
        else:
            self.shortcut = DWSepConvLayer(
                c1,
                c2,
                k,
                s,
                act=act,
                pw_act="noop",
                norm=norm,
            )

    def forward(self, x):
        return self.shortcut(x)


class BottleneckLayer(nn.Sequential):
    def __init__(
        self,
        c1,
        c2=None,
        k=3,
        s=1,
        act="relu",
        norm="bn2d",
    ):
        c2 = c2 or c1
        super().__init__(
            ConvLayer(c1, c2, k=k, s=s, act=act, norm=norm),
            ConvLayer(c2, c2, act="noop", norm=norm),
        )


class LightBottleneckLayer(nn.Sequential):
    def __init__(
        self,
        c1,
        c2=None,
        f=0.5,
        k=3,
        s=1,
        g=1,
        act="relu",
        norm="bn2d",
    ):
        c2 = c2 or c1
        c_ = math.ceil(f * c1)
        super().__init__(
            Conv1x1Layer(
                c1,
                c_,
                act=act,
                norm=norm,
            )
            if f != 1
            else nn.Identity(),
            ConvLayer(c_, c_, k, s, g=g, act=act, norm=norm),
            Conv1x1Layer(c_, c2, act="noop", norm=norm),
        )


class DWLightBottleneckLayer(LightBottleneckLayer):
    def __init__(
        self,
        c1,
        c2=None,
        f=4,
        k=3,
        s=1,
        act="relu",
        norm="bn2d",
    ):
        g = math.ceil(f * c1)
        super().__init__(c1, c2, f, k, s, g, act, norm)


class DenseShortcutLayer(nn.Module):
    def __init__(
        self,
        c1,
        f=32,
        act="relu",
        norm="bn2d",
    ):
        super().__init__()
        self.block = ConvLayer(c1, f, act=act, norm=norm, pre_normact=True)

    def forward(self, x):
        out = self.block(x)
        return torch.cat([x, out], dim=1)


class ScaledResidual(nn.Module):
    def __init__(self, *layers, shortcut=None, apply_shortcut=True):
        super().__init__()
        self.shortcut = nn.Identity() if shortcut is None else shortcut
        self.residual = nn.Sequential(*layers)
        self.apply_shortcut = int(apply_shortcut)
        self.gamma = nn.Parameter(torch.zeros(1)) if apply_shortcut else 1

    def forward(self, x):
        return self.apply_shortcut * self.shortcut(x) + self.gamma * self.residual(x)


class ChannelShuffle(nn.Module):
    def __init__(self, g):
        super().__init__()
        self.g = g
        self.block = Rearrange("n (g d) h w -> n (d g) h w", g=g)

    def forward(self, x):
        return self.block(x)


---
src/visionlab/models/blocks/posenc.py
---
import torch
from torch import nn


class LearnablePositionEnc(nn.Module):
    def __init__(self, sizes):
        super().__init__()
        self.pos_enc = nn.Parameter(torch.zeros(1, *sizes))
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Parameter):
            nn.init.trunc_normal_(m, std=0.2)

    def forward(self, x):
        return x + self.pos_enc


---
src/visionlab/models/blocks/__init__.py
---
from .attentions import (
    MultiScaleSAV2,
)
from .convs import (
    ConvMixerBlock,
    FusedMBConvBlock,
    GhostBottleneckBlock,
    LightResNetBlock,
    MBConvBlock,
    MobileNetBlock,
    ResNetBlock,
    ResNeXtBlock,
    ShuffleBlock,
)
from .heads import (
    ConvHead,
    FCHead,
)
from .layers import (
    BasicFCLayer,
    ConvLayer,
    FlattenLayer,
    SoftSplit,
    UnflattenLayer,
)
from .necks import (
    AvgPool,
    PatchNorm,
    SequencePooling,
)
from .posenc import (
    LearnablePositionEnc,
)
from .stems import (
    SPT,
    ConvMpPatch,
)
from .transformers import (
    T2TBlock,
    TransformerEncoder,
    TransformerEncoderMultiScale,
)


---
src/visionlab/models/blocks/stems.py
---
import torch.nn as nn
from einops.layers.torch import Rearrange

from .layers import (
    BasicFCLayer,
    ConvLayer,
    Shifting,
    SoftSplit,
    UnflattenLayer,
)
from .transformers import (
    T2TBlock,
)


class T2TPatchV2(nn.Module):
    def __init__(
        self,
        c1,
        c2,
        h,
        f=1,
        kn=3,
        sn=1,
        heads=1,
        act="gelu",
        drop=0.0,
        **kwargs,
    ):
        super().__init__()
        layers = []
        last = len(sn) - 1
        c_ = int(f * c2)

        for i, (k, s) in enumerate(zip(kn, sn)):
            layers.extend(
                [
                    SoftSplit(c1, c2 if i == last else c_, k, s)
                    if i == 0
                    else T2TBlock(
                        c_,
                        c2 if i == last else c_,
                        h,
                        f=f,
                        k=k,
                        s=s,
                        heads=heads,
                        act=act,
                        drop=drop,
                    ),
                ]
            )
            h //= s
        layers.append(UnflattenLayer(h=h))
        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)


class MDMLPPatch(nn.Module):
    def __init__(
        self,
        c1,
        c2,
        k,
        s,
        act="noop",
        norm="noop",
        drop=0.0,
        **kwargs,
    ):
        super().__init__()
        self.block = nn.Sequential(
            nn.Unfold(kernel_size=k, stride=s, padding=(k - 1) // 2),
            Rearrange(
                "n (c d) p -> n c p d",
                c=c1,
                d=k**2,
            ),
            BasicFCLayer(
                k**2,
                c2,
                act=act,
                norm=norm,
                drop=drop,
                pre_norm=False,
            ),
        )

    def forward(self, x):
        return self.block(x)


class ConvMpPatch(nn.Module):
    def __init__(
        self,
        c1,
        c2,
        k=3,
        s=1,
        act="noop",
        norm="noop",
        mp=True,
        **kwargs,
    ):
        super().__init__()
        self.block = nn.Sequential(
            ConvLayer(c1, c2, k, s, act=act, norm=norm),
            nn.MaxPool2d(
                kernel_size=kwargs.get("mp_k", 3),
                stride=kwargs.get("mp_s", 2),
                padding=kwargs.get("mp_p", 1),
            )
            if mp
            else nn.Identity(),
        )

    def forward(self, x):
        return self.block(x)


class SPT(nn.Module):
    def __init__(self, c1, c2, patch_size=16):
        super().__init__()

        self.shifting = Shifting(patch_size // 2)

        patch_dim = (c1 * 5) * (patch_size**2)

        self.patching = nn.Sequential(
            Rearrange(
                "b c (h p1) (w p2) -> b (h w) (p1 p2 c)", p1=patch_size, p2=patch_size
            ),
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, c2),
        )

    def forward(self, x):
        out = self.shifting(x)
        out = self.patching(out)

        return out


---
src/visionlab/models/blocks/mlps.py
---
from einops import rearrange
from einops.layers.torch import Rearrange
from torch import nn

from ...utils.ml import swap_dims
from .layers import (
    ExpansionFCLayer,
    FlattenLayer,
    ScaledResidual,
)


class MLPTokenMixerBlock(nn.Module):
    def __init__(
        self,
        in_channels,
        num_patches,
        expansion=4,
        dropout=0.0,
        act="gelu",
    ):
        super().__init__()
        self.block = ScaledResidual(
            FlattenLayer(
                norm_dims=in_channels,
                norm="ln",
                norm_order="pre",
            ),
            Rearrange("n p c -> n c p"),
            ExpansionFCLayer(
                num_patches,
                expansion,
                act=act,
                dropout=dropout,
                norm="noop",
            ),
            Rearrange("n c p -> n p c"),
        )

    def forward(self, x):
        return rearrange(
            self.block(x),
            "n (h w) c -> n c h w",
            w=x.shape[-1],
        )


class MLPChannelMixerBlock(nn.Module):
    def __init__(
        self,
        in_channels,
        expansion=0.5,
        dropout=0.0,
        act="gelu",
    ):
        super().__init__()
        self.block = ScaledResidual(
            FlattenLayer(
                norm_dims=in_channels,
                norm="ln",
                norm_order="pre",
            ),
            ExpansionFCLayer(
                in_channels,
                expansion,
                act=act,
                dropout=dropout,
                norm="noop",
            ),
        )

    def forward(self, x):
        return rearrange(
            self.block(x),
            "n (h w) c -> n c h w",
            w=x.shape[-1],
        )


class MLPMixerBlock(nn.Module):
    def __init__(
        self,
        in_channels,
        num_patches,
        tm_expansion=4,
        cm_expansion=0.5,
        dropout=0.0,
        act="gelu",
    ):
        super().__init__()
        self.num_patches = num_patches
        self.token_mixer = MLPTokenMixerBlock(
            in_channels,
            num_patches,
            tm_expansion,
            dropout=dropout,
            act=act,
        )
        self.channel_mixer = MLPChannelMixerBlock(
            in_channels,
            cm_expansion,
            dropout=dropout,
            act=act,
        )

    def forward(self, x):
        return self.channel_mixer(self.token_mixer(x))


class MLPDimMixerBlock(nn.Module):
    def __init__(
        self,
        norm_channels,
        mixing_channels,
        dim=-1,
        expansion=4,
        dropout=0.0,
        act="gelu",
    ):
        super().__init__()
        self.norm = nn.LayerNorm(norm_channels)
        self.dim = dim
        self.block = ExpansionFCLayer(
            mixing_channels,
            expansion,
            act=act,
            dropout=dropout,
            norm="noop",
        )

    def forward(self, x):
        # x: [n, d1, d2, ... in_dims]
        y = self.norm(x)
        y = swap_dims(y, self.dim, -1)
        y = self.block(y)
        y = swap_dims(y, self.dim, -1)
        return x + y


class MDMLPMixerBlock(nn.Module):
    def __init__(
        self,
        in_channels,
        norm_channels,
        patch_num_h,
        patch_num_w,
        expansion=4,
        dropout=0.0,
        act="gelu",
    ):
        super().__init__()
        self.num_patches_h = patch_num_h
        self.num_patches_w = patch_num_w
        l_dims = [2, 3, 1, 4]
        l_mixing_channels = [
            patch_num_h,
            patch_num_w,
            in_channels,
            norm_channels,
        ]
        self.block = nn.Sequential(
            *[
                MLPDimMixerBlock(
                    norm_channels=norm_channels,
                    mixing_channels=l_mixing_channels[i],
                    dim=l_dims[i],
                    expansion=expansion,
                    dropout=dropout,
                    activation=act,
                )
                for i in range(len(l_dims))
            ],
        )

    def forward(self, x):
        # x: [n, c, p, d]
        # p = num_patches, d=patch_sz**2 (i.e. a flattened patch)
        x = rearrange(
            x,
            "n c (h w) d -> n c h w d",
            h=self.num_patches_h,
            w=self.num_patches_w,
        )
        return rearrange(self.block(x), "n c h w d -> n c (h w) d")


---
src/visionlab/models/blocks/heads.py
---
from typing import List

import torch.nn as nn

from .layers import (
    BasicFCLayer,
    Conv1x1Layer,
)


class ConvHead(nn.Module):
    def __init__(
        self,
        c1: int,
        cn: List[int],
        act="relu",
        last_act="noop",
        norm="bn2d",
    ):
        super().__init__()
        prev = c1
        layers = []
        last = len(cn) - 1
        for idx, c_ in enumerate(cn):
            layers.append(
                Conv1x1Layer(
                    prev,
                    c_,
                    act=act if idx != last else last_act,
                    norm=norm if idx != last else "noop",
                )
            )
            prev = c_
        layers.append(nn.Flatten())
        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)


class FCHead(nn.Module):
    def __init__(
        self,
        c1: int,
        cn: List[int],
        act="relu",
        last_act="noop",
        norm="bn1d",
        drop: float = 0.0,
    ):
        super().__init__()
        prev = c1
        layers = []
        last = len(cn) - 1
        for idx, c_ in enumerate(cn):
            layers.append(
                BasicFCLayer(
                    prev,
                    c_,
                    act=act if idx != last else last_act,
                    drop=drop if idx != last else 0.0,
                    norm=norm if idx != last else "noop",
                ),
            )
            prev = c_

        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)


---
src/visionlab/models/blocks/attentions.py
---
import math
from typing import Literal

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange
from einops.layers.torch import Rearrange

from .layers import (
    BasicFCLayer,
    Conv1x1Layer,
    ConvLayer,
)


class MultiScaleSAV2(nn.Module):
    def __init__(
        self,
        c1,
        h,
        k_q=1,
        k_kv=1,
        s_q=1,
        s_kv=1,
        heads=2,
        drop=0.0,
        **kwargs,
    ):
        super().__init__()
        self.heads = heads
        self.h = h
        self.drop = drop
        self.pool_q = nn.Sequential(
            nn.MaxPool2d(k_q, s_q, (k_q - 1) //
                         2) if k_q > 1 else nn.Identity(),
            Rearrange("n c h w -> n (h w) c"),
        )
        self.pool_kv = nn.Sequential(
            nn.MaxPool2d(k_kv, s_kv, (k_kv - 1) //
                         2) if k_kv > 1 else nn.Identity(),
            Rearrange("n c h w -> n (h w) c"),
        )
        self.qkv = nn.Sequential(
            nn.Linear(
                c1,
                c1 * 3,
                bias=kwargs.get("qkv_bias", False),
            ),
            Rearrange(
                "n (h w) (a m d) -> a (n m) d h w",
                m=heads,
                a=3,
                h=h,
            ),
        )

    def forward(self, x):
        n, p, c = x.shape
        q, k, v = self.qkv(x)  # [n*m, c', h, w]
        q = self.pool_q(q)  # [n*m, (h'*w'), c']
        k = self.pool_kv(k)  # [n*m, (h''*w''), c']
        v = self.pool_kv(v)  # [n*m, (h''*w''), c']
        out = F.scaled_dot_product_attention(
            q,
            k,
            v,
            dropout_p=self.drop,
        )  # [n*m, (h'*w'), c']
        return rearrange(out, "(n m) p d -> n p (m d)", m=self.num_heads)


class MultiHeadSA(nn.Module):
    def __init__(
        self,
        c1,
        heads=2,
        drop=0.0,
        **kwargs,
    ):
        super().__init__()
        self.mha = nn.MultiheadAttention(
            c1,
            heads,
            dropout=drop,
            batch_first=True,
            **kwargs,
        )

    def forward(self, x):
        return self.mha(x, x, x)[0]  # [n, p, c]


class ECABlockV2(nn.Module):
    def __init__(self, c1, gamma=2, bias=1):
        super().__init__()
        k = int(abs(math.log(c1, 2) + bias) / gamma)
        k = k if k % 2 else k + 1

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.block = nn.Conv1d(2, 1, k, padding="same")
        self.act = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.avg_pool(x).squeeze(-1).transpose(-1, -2)
        max_out = self.max_pool(x).squeeze(-1).transpose(-1, -2)
        weights = self.block(torch.cat([avg_out, max_out], dim=1))
        weights = self.act(weights).transpose(-1, -2).unsqueeze(-1)
        return x * weights


class ECABlock(nn.Module):
    def __init__(self, c1, gamma=2, bias=1):
        super().__init__()
        k = int(abs(math.log(c1, 2) + bias) / gamma)
        k = k if k % 2 else k + 1

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.block = nn.Conv1d(1, 1, k, padding="same")
        self.act = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.avg_pool(x).squeeze(-1).transpose(-1, -2)
        weights = self.block(avg_out)
        weights = self.act(weights).transpose(-1, -2).unsqueeze(-1)
        return x * weights


class ChannelAttentionBlock(nn.Module):
    def __init__(self, c1, f=0.8, use_conv=True):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        block_cls = Conv1x1Layer if use_conv else BasicFCLayer
        self.block = nn.Sequential(
            block_cls(
                c1,
                math.ceil(f * c1),
                act="relu",
                norm="noop",
            ),
            block_cls(
                math.ceil(f * c1),
                c1,
                act="noop",
                norm="noop",
            ),
        )
        self.act = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.block(self.avg_pool(x))
        max_out = self.block(self.max_pool(x))
        return x * self.act(avg_out + max_out)


class SpatialAttentionBlock(nn.Module):
    def __init__(self):
        super().__init__()
        self.block = ConvLayer(2, 1, act="sigmoid", norm="noop")

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        return x * self.block(torch.cat([avg_out, max_out], dim=1))


class SEBlock(nn.Module):
    def __init__(self, c1, f=0.25, use_conv=True):
        super().__init__()
        self.pool = nn.AdaptiveAvgPool2d(1)

        block_cls = Conv1x1Layer if use_conv else BasicFCLayer
        self.block = nn.Sequential(
            block_cls(c1, math.ceil(f * c1), act="relu", norm="noop"),
            block_cls(math.ceil(f * c1), c1, act="hsigmoid", norm="noop"),
        )

    def forward(self, x):
        weights = self.pool(x)
        weights = self.block(weights).view(x.size(0), -1, 1, 1)
        return x * weights


class CBAMBlock(nn.Module):
    def __init__(
        self,
        c1,
        ca: Literal["eca", "ecav2", "cbam", "se"] = "eca",
        **ca_kwargs,
    ):
        super().__init__()
        if ca == "eca":
            self.ca = ECABlock(c1, **ca_kwargs)
        elif ca == "ecav2":
            self.ca = ECABlockV2(c1, **ca_kwargs)
        elif ca == "cbam":
            self.ca = ChannelAttentionBlock(c1, **ca_kwargs)
        elif ca == "se":
            self.ca = SEBlock(c1, **ca_kwargs)
        else:
            raise ValueError(f"unknown channel attn type: {ca}")

        self.sa = SpatialAttentionBlock()

    def forward(self, x):
        x = self.ca(x)
        return self.sa(x)


---
src/visionlab/models/blocks/transformers.py
---
import torch.nn as nn
from einops.layers.torch import Rearrange

from ..norm_factory import Norms
from .attentions import MultiHeadSA, MultiScaleSAV2
from .layers import (
    ApplyNorm,
    ExpansionFCLayer,
    ScaledResidual,
    SoftSplit,
    UnflattenLayer,
)


class T2TBlock(nn.Sequential):
    def __init__(
        self,
        c1,
        c2,
        h,
        f=1,
        k=3,
        s=1,
        heads=1,
        act="gelu",
        drop=0.0,
    ):
        super().__init__(
            TransformerEncoder(c1, c1, f, heads, act, drop),
            UnflattenLayer(h=h),
            SoftSplit(c1, c2, k=k, s=s),
        )


class TransformerEncoderMultiScale(nn.Module):
    def __init__(
        self,
        c1,
        c2,
        h,
        f=2,
        k_q=1,
        k_kv=1,
        s_q=1,
        s_kv=1,
        heads=2,
        act="gelu",
        drop=0.0,
        pre_norm=True,
    ):
        super().__init__()
        msa = MultiScaleSAV2(
            c1,
            h,
            k_q,
            k_kv,
            s_q,
            s_kv,
            heads,
            drop,
        )
        msa_shortcut = nn.Sequential(
            Rearrange("n (h w) c -> n c h w", h=h),
            msa.pool_q,
        )

        mlp_shortcut = nn.Sequential(
            Rearrange("n p c -> n c p"),
            nn.Conv1d(c1, c2, 1),
            Rearrange("n c p -> n p c"),
        )

        self.block = nn.Sequential(
            ScaledResidual(
                ApplyNorm(Norms.get("ln", c1), msa, pre_norm),
                shortcut=msa_shortcut,
            ),
            ScaledResidual(
                ExpansionFCLayer(c1, c2, f, act, "ln", drop, pre_norm),
                shortcut=mlp_shortcut,
            ),
        )

    def forward(self, x):
        return self.block(x)


class TransformerEncoder(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        f=2,
        heads=2,
        act="gelu",
        drop=0.0,
        pre_norm=True,
    ):
        super().__init__()
        c2 = c2 or c1
        mlp_shortcut = nn.Sequential(
            Rearrange("n p c -> n c p"),
            nn.Conv1d(c1, c2, 1),
            Rearrange("n c p -> n p c"),
        )
        self.block = nn.Sequential(
            ScaledResidual(
                ApplyNorm(
                    Norms.get("ln", c1),
                    MultiHeadSA(c1, heads, drop),
                    pre_norm,
                )
            ),
            ScaledResidual(
                ExpansionFCLayer(c1, c2, f, act, "ln", drop, pre_norm),
                shortcut=mlp_shortcut,
            ),
        )

    def forward(self, x):
        return self.block(x)  # [n, p, c]


---
src/visionlab/models/blocks/convs.py
---
import math

import torch
import torch.nn as nn

from ...utils.ml import make_divisible
from ..act_factory import Acts
from ..attn_factory import Attentions
from .layers import (
    BottleneckLayer,
    ChannelShuffle,
    Conv1x1Layer,
    ConvLayer,
    DenseShortcutLayer,
    DWConvLayer,
    DWLightBottleneckLayer,
    DWSepShortcutLayer,
    GhostLayer,
    LightBottleneckLayer,
    LightShortcutLayer,
    ScaledResidual,
    Shortcut3x3Layer,
    ShortcutLayer,
)


class ShuffleBlock(nn.Module):
    def __init__(self, c1, c2=None, f=2, k=3, s=1, g=3, act="relu"):
        super().__init__()
        self.s = s
        c2 = c2 or c1
        c2 = c2 // 2 if s != 1 else c2
        c_ = make_divisible(f * c1, 4)

        self.block = nn.Sequential(
            Conv1x1Layer(c1, c_, g=g, act=act),
            ChannelShuffle(g),
            ConvLayer(c_, c_, k=k, s=s, g=g, act="noop"),
            Conv1x1Layer(c_, c2, g=g, act="noop"),
        )
        self.shortcut = LightShortcutLayer(c1, c2, s=s, g=g)

    def forward(self, x):
        out = self.block(x)
        shortcut = self.shortcut(x)

        if self.s != 1:
            return torch.cat([shortcut, out], 1)
        return shortcut + out


class FusedMBConvBlock(nn.Module):
    def __init__(self, c1, c2, f=4, k=3, s=1, act="silu"):
        super().__init__()

        c_ = math.ceil(f * c1)

        self.block = ScaledResidual(
            ConvLayer(c1, c_, k=k, s=s, act=act) if f != 1 else nn.Identity(),
            Conv1x1Layer(c_, c2, act="noop") if f != 1 else nn.Identity(),
            shortcut=ShortcutLayer(c1, c2, s),
            apply_shortcut=(s == 1 and c1 == c2),
        )

    def forward(self, x):
        return self.block(x)


class MBConvBlock(nn.Module):
    def __init__(
        self,
        c1,
        c2,
        f=4,
        k=3,
        s=1,
        act="hswish",
        attn="se",
    ):
        super().__init__()
        c_ = math.ceil(f * c1)

        self.block = ScaledResidual(
            Conv1x1Layer(c1, c_, act=act) if f != 1 else nn.Identity(),
            DWConvLayer(c_, c_, k=k, s=s, act=act),
            Attentions.get(attn, c_),
            Conv1x1Layer(c_, c2, act="noop"),
            shortcut=ShortcutLayer(c1, c2, s),
            apply_shortcut=(s == 1 and c1 == c2),
        )

    def forward(self, x):
        return self.block(x)


class MobileNetBlock(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        f=2,
        k=3,
        s=1,
        act="relu",
        norm="bn2d",
        attn="noop",
    ):
        super().__init__()
        c2 = c2 or c1
        self.block = nn.Sequential(
            ScaledResidual(
                DWLightBottleneckLayer(c1, c2, f, k, s, act, norm),
                Attentions.get(attn, c2),
                shortcut=ShortcutLayer(c1, c2, s),
                apply_shortcut=(s == 1 and c1 == c2),
            ),
            Acts.get(act),
        )

    def forward(self, x):
        return self.block(x)


class GhostBottleneckBlock(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        f=3,
        k=3,
        s=1,
        act="relu",
        norm="bn2d",
        attn="noop",
    ):
        super().__init__()
        c2 = c2 or c1
        c_ = make_divisible(f * c1, 4)
        downsample = (
            nn.Identity()
            if s == 1
            else DWConvLayer(
                c_,
                c_,
                k,
                s,
                act="noop",
            )
        )
        self.block = ScaledResidual(
            GhostLayer(c1, c_, k=k, act=act, norm=norm),
            downsample,
            Attentions.get(attn, c_),
            GhostLayer(c_, c2, k=k, act="noop", norm=norm),
            shortcut=DWSepShortcutLayer(c1, c2, k, s, act, norm),
        )

    def forward(self, x):
        return self.block(x)


class StackedDenseBlock(nn.Module):
    def __init__(
        self,
        n,
        c1,
        f=32,
        act="relu",
        norm="bn2d",
    ):
        super().__init__()
        layers = []
        for _ in range(n):
            layers += [DenseShortcutLayer(c1, f, act=act, norm=norm)]
            c1 += f
        self.block = nn.Sequential(*layers)
        self.c2 = c1

    def forward(self, x):
        return self.block(x)


class LightResNetBlock(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        k=3,
        s=1,
        act="relu",
        norm="bn2d",
        attn="noop",
    ):
        super().__init__()
        c2 = c2 or c1
        self.block = nn.Sequential(
            ScaledResidual(
                nn.MaxPool2d(2, s) if s > 1 else nn.Identity(),
                BottleneckLayer(c1, c2, k, 1, act, norm),
                Attentions.get(attn, c2),
                shortcut=ShortcutLayer(c1, c2, s),
            ),
        )

    def forward(self, x):
        return self.block(x)


class ResNetBlock(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        k=3,
        s=1,
        act="relu",
        norm="bn2d",
        attn="noop",
    ):
        super().__init__()
        c2 = c2 or c1

        self.block = nn.Sequential(
            ScaledResidual(
                BottleneckLayer(c1, c2, k, s, act, norm),
                Attentions.get(attn, c2),
                shortcut=Shortcut3x3Layer(c1, c2, s),
            ),
            Acts.get(act),
        )

    def forward(self, x):
        return self.block(x)


class ResNeXtBlock(nn.Module):
    def __init__(
        self,
        c1,
        c2=None,
        f=0.5,
        k=3,
        s=1,
        g=1,
        act="relu",
        norm="bn2d",
        attn="noop",
    ):
        super().__init__()
        c2 = c2 or c1

        self.block = nn.Sequential(
            ScaledResidual(
                LightBottleneckLayer(c1, c2, f, k, s, g, act, norm),
                Attentions.get(attn, c2),
                shortcut=ShortcutLayer(c1, c2, s),
            ),
            Acts.get(act),
        )

    def forward(self, x):
        return self.block(x)


class ConvMixerBlock(nn.Module):
    def __init__(
        self,
        c1,
        k=7,
        s=1,
        act="gelu",
        norm="bn2d",
    ):
        super().__init__()
        self.block = nn.Sequential(
            ScaledResidual(
                ConvLayer(
                    c1,
                    c1,
                    k,
                    s,
                    g=c1,
                    act=act,
                    norm=norm,
                ),
                ShortcutLayer(c1, c1, s),
            ),
            Conv1x1Layer(c1, c1, act=act, norm=norm),
        )

    def forward(self, x):
        return self.block(x)


class ConvTokenMixerBlock(nn.Module):
    def __init__(
        self,
        c1,
        seq,
        f=4,
        act="gelu",
        drop=0.0,
    ):
        super().__init__()
        self.block = ScaledResidual(
            nn.LayerNorm(c1),
            nn.Conv1d(seq, int(seq * f), 1),
            nn.Dropout(drop),
            nn.Conv1d(int(seq * f), seq, 1),
            nn.Dropout(drop),
            Acts.get(act),
        )

    def forward(self, x):
        return self.block(x)  # [n, p, c]


class InceptionBlock(nn.Module):
    def __init__(
        self,
        c1,
        cn,
        fn,
        kn,
        act="relu",
        norm="bn2d",
        last_conv1x1=False,
    ):
        """
        l_red_channels: list of reduction channels
        l_out_channels: list of output channels
        l_kernel_sizes: list of kernel sizes
        """
        super().__init__()
        self.branches = nn.ModuleList()
        for i, (f, c2, k) in enumerate(zip(fn, cn, kn)):
            if k == 1:
                layers = (
                    []
                    if f is None
                    else [
                        nn.MaxPool2d(3, 1, 1),
                    ]
                )
                layers.append(Conv1x1Layer(c1, c2, act=act))
                branch = nn.Sequential(*layers)
            else:
                branch = LightBottleneckLayer(
                    c1,
                    c2,
                    f,
                    k,
                    act=act,
                    norm=norm,
                )
            self.branches.append(branch)

    def forward(self, x):
        return torch.cat([branch(x) for branch in self.branches], dim=1)


---
src/visionlab/models/nets/yolo.py
---



---
src/visionlab/models/nets/lenet.py
---
import torch
import torch.nn as nn
from einops.layers.torch import Rearrange
from visionlab.models.blocks.attentions import (
    Attentions,
    MultiHead_SA,
)
from visionlab.models.blocks.composites import (
    ConvMixerBlock,
    DenseNetBlock,
    InceptionBlock,
    MLPMixerBlock,
    ResNetBlock,
)
from visionlab.models.blocks.core import (
    ConvLayer,
)
from visionlab.utils.registry import load_obj
from omegaconf import DictConfig


class LeNet5(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()

        self.layer1 = nn.Sequential(
            ConvLayer(3, 6, kernel_size=5, padding=0),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.layer2 = nn.Sequential(
            ConvLayer(6, 16, kernel_size=5, padding=0),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        self.head = load_obj(config.model.head.class_name)(
            in_features=16,
            **config.model.head.params,
        )

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        return self.head(x)


class LeNetV2(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()

        self.stem = nn.Sequential(
            ConvLayer(3, 6, kernel_size=5, padding=0),
        )

        self.trunk = nn.Sequential(
            ResNetBlock(6, 12, stride=2),
            InceptionBlock(
                12,
                [6, 2],
                [8, 4],
                [3, 5],
            ),
        )

        self.head = load_obj(config.model.head.class_name)(
            in_features=12,
            **config.model.head.params,
        )

    def forward(self, x):
        x = self.stem(x)
        x = self.trunk(x)
        return self.head(x)


class LeNetV3(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()

        self.stem = nn.Sequential(
            ConvLayer(3, 6, kernel_size=5, padding=0),
        )

        self.trunk = nn.Sequential(
            MultiHead_SA(6),
            ResNetBlock(6, 6, attn=Attentions.ECA()),
            ConvLayer(6, 12, stride=2, padding=1),
            ResNetBlock(12, 12),
            ConvLayer(12, 24, stride=2, padding=1),
        )

        self.head = load_obj(config.model.head.class_name)(
            in_features=24,
            **config.model.head.params,
        )

    def forward(self, x):
        x = self.stem(x)
        x = self.trunk(x)
        return self.head(x)


class LeNetV4(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()

        self.stem = nn.Sequential(
            ConvLayer(3, 6, kernel_size=5, padding=0),
        )

        dense_1 = DenseNetBlock(6, 12)
        dense_2 = DenseNetBlock(dense_1.out_channels, 12)

        self.trunk = nn.Sequential(
            MultiHead_SA(6),
            dense_1,
            nn.MaxPool2d(kernel_size=2, stride=2),
            dense_2,
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        self.neck = load_obj(config.model.neck.class_name)(
            in_channels=dense_2.out_channels,
            **config.model.neck.get("params", {}),
        )

        self.head = load_obj(config.model.head.class_name)(
            in_features=dense_2.out_channels,
            **config.model.head.params,
        )

    def forward(self, x):
        x = self.stem(x)
        x = self.trunk(x)
        return self.head(self.neck(x))


class LeNetV5(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()
        self.stem = load_obj(config.model.stem.class_name)(
            in_channels=3,
            out_channels=128,
            **config.model.stem.get("params", {}),
        )
        self.trunk = nn.Sequential(
            ConvMixerBlock(128, kernel_size=7),
            ConvMixerBlock(128, kernel_size=7),
            ConvMixerBlock(128, kernel_size=7),
            ConvMixerBlock(128, kernel_size=7),
        )
        self.head = load_obj(config.model.head.class_name)(
            in_features=128,
            **config.model.head.params,
        )

    def forward(self, x):
        return self.head(self.trunk(self.stem(x)))


class LeNetV6(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()
        self.stem = load_obj(config.model.stem.class_name)(
            in_channels=3,
            out_channels=128,
            **config.model.stem.get("params", {}),
        )
        patch_size = config.model.stem.params.get("patch_size", 1)
        num_patches = (config.model.input_size // patch_size) ** 2
        self.trunk = nn.Sequential(
            MLPMixerBlock(128, num_patches, 2, 0.5),
            MLPMixerBlock(128, num_patches, 2, 0.5),
            MLPMixerBlock(128, num_patches, 2, 0.5),
            MLPMixerBlock(128, num_patches, 2, 0.5),
            MLPMixerBlock(128, num_patches, 2, 0.5),
            MLPMixerBlock(128, num_patches, 2, 0.5),
        )

        self.neck = load_obj(config.model.neck.class_name)(
            in_channels=128,
            **config.model.neck.get("params", {}),
        )

        self.head = load_obj(config.model.head.class_name)(
            in_features=128,
            **config.model.head.get("params", {}),
        )

    def forward(self, x):
        return self.head(self.neck(self.trunk(self.stem(x))))


---
src/visionlab/models/nets/mlpmixer.py
---
import torch.nn as nn
from einops.layers.torch import Reduce
from visionlab.models.blocks.composites import (
    MDMLPMixerBlock,
)
from visionlab.utils.registry import load_obj
from omegaconf import DictConfig


class MDMLPTiny(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()
        n_layers = 6
        embed_sz = 64

        self.stem = load_obj(config.model.stem.class_name)(
            in_channels=3,
            out_channels=embed_sz,
            **config.model.stem.get("params", {}),
        )

        patch_num_h = self.stem.patch_num_h
        patch_num_w = self.stem.patch_num_w

        self.trunk = nn.Sequential(
            *[
                MDMLPMixerBlock(
                    in_channels=3,
                    norm_channels=embed_sz,
                    dropout=0.2,
                    patch_num_h=patch_num_h,
                    patch_num_w=patch_num_w,
                )
                for _ in range(n_layers)
            ],
        )

        self.neck = Reduce("n c p d -> n d", "mean")

        self.head = load_obj(config.model.head.class_name)(
            in_features=embed_sz,
            **config.model.head.get("params", {}),
        )

    def forward(self, x):
        return self.head(self.neck(self.trunk(self.stem(x))))


---
src/visionlab/models/nets/convtiny.py
---
import torch.nn as nn
from visionlab.models.blocks.core import (
    ConvLayer,
    ScaledResidual,
)
from visionlab.utils.ml import init_linear
from visionlab.utils.registry import load_obj
from omegaconf import DictConfig


class Block(ScaledResidual):
    def __init__(self, channels, kernel_size=3, stride=1, mult=4):
        mid_channels = channels * mult
        kernel_size = kernel_size + stride - 1
        super().__init__(
            ConvLayer(
                channels,
                mid_channels,
                kernel_size,
                stride=stride,
                groups=channels,
                padding=(kernel_size - 1) // 2,
            ),
            ConvLayer(
                mid_channels,
                channels,
                1,
            ),
            shortcut=nn.AvgPool2d(stride) if stride > 1 else None,
        )


class Stage(nn.Sequential):
    def __init__(
        self,
        channels,
        num_blocks,
        kernel_size=3,
        stride=1,
        mult=4,
    ):
        super().__init__(
            Block(channels, kernel_size, stride, mult),
            *[Block(channels, kernel_size, 1, mult)
              for _ in range(num_blocks - 1)],
        )


class StageStack(nn.Sequential):
    def __init__(
        self,
        channels,
        num_blocks,
        strides,
        kernel_size=3,
        mult=4,
    ):
        super().__init__(
            *[
                Stage(channels, num_blocks, kernel_size, stride, mult)
                for stride in strides
            ]
        )


class ConvTiny(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()
        embed_sz = 128
        # inter_channels = 48
        strides = [1, 2, 2, 2]

        self.stem = nn.Conv2d(3, embed_sz, 3, padding=1, bias=False)
        self.trunk = StageStack(embed_sz, 2, strides, 3, 4)
        self.neck = load_obj(config.model.neck.class_name)(
            in_channels=embed_sz,
            **config.model.neck.get("params", {}),
        )

        self.head = load_obj(config.model.head.class_name)(
            in_channels=embed_sz,
            **config.model.head.params,
        )

        self.apply(init_linear)

    def forward(self, x):
        return self.head(self.neck(self.trunk(self.stem(x))))


---
src/visionlab/models/nets/t2t.py
---
import math

import numpy as np
import torch
from einops import rearrange, repeat
from einops.layers.torch import Rearrange
from torch import nn


class Residual(nn.Module):
    def __init__(self, *layers, shortcut=None):
        super().__init__()
        self.shortcut = nn.Identity() if shortcut is None else shortcut
        self.residual = nn.Sequential(*layers)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return self.shortcut(x) + self.gamma * self.residual(x)


class SelfAttention(nn.Module):
    def __init__(self, dim, head_dim, heads=8, p_drop=0.0):
        super().__init__()
        inner_dim = head_dim * heads
        self.head_shape = (heads, head_dim)
        self.scale = head_dim**-0.5

        self.to_keys = nn.Linear(dim, inner_dim)
        self.to_queries = nn.Linear(dim, inner_dim)
        self.to_values = nn.Linear(dim, inner_dim)
        self.unifyheads = nn.Linear(inner_dim, dim)

        self.drop = nn.Dropout(p_drop)

    def forward(self, x):
        q_shape = x.shape[:-1] + self.head_shape

        keys = (
            self.to_keys(x).view(q_shape).transpose(1, 2)
        )  # move head forward to the batch dim
        queries = self.to_queries(x).view(q_shape).transpose(1, 2)
        values = self.to_values(x).view(q_shape).transpose(1, 2)

        att = queries @ keys.transpose(-2, -1)
        att = (att * self.scale).softmax(dim=-1)

        out = att @ values
        out = out.transpose(1, 2).contiguous().flatten(2)  # move head back
        out = self.unifyheads(out)
        out = self.drop(out)
        return out


class FeedForward(nn.Sequential):
    def __init__(self, dim, mlp_mult=4, p_drop=0.0):
        hidden_dim = dim * mlp_mult
        super().__init__(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(p_drop),
        )


class TransformerBlock(nn.Sequential):
    def __init__(self, dim, head_dim, heads, mlp_mult=4, p_drop=0.0):
        super().__init__(
            Residual(nn.LayerNorm(dim), SelfAttention(
                dim, head_dim, heads, p_drop)),
            Residual(nn.LayerNorm(dim), FeedForward(
                dim, mlp_mult, p_drop=p_drop)),
        )


class SoftSplit(nn.Module):
    def __init__(self, in_channels, dim, kernel_size=3, stride=2):
        super().__init__()
        padding = (kernel_size - 1) // 2
        self.unfold = nn.Unfold(kernel_size, stride=stride, padding=padding)
        self.project = nn.Linear(in_channels * kernel_size**2, dim)

    def forward(self, x):
        out = self.unfold(x).transpose(1, 2)
        out = self.project(out)
        return out


class Reshape(nn.Module):
    def __init__(self, shape):
        super().__init__()
        self.shape = shape

    def forward(self, x):
        out = x.transpose(1, 2).unflatten(2, self.shape)
        return out


class T2TBlock(nn.Sequential):
    def __init__(
        self,
        image_size,
        token_dim,
        embed_dim,
        heads=1,
        mlp_mult=1,
        stride=2,
        p_drop=0.0,
    ):
        super().__init__(
            TransformerBlock(token_dim, token_dim // heads,
                             heads, mlp_mult, p_drop),
            Reshape((image_size, image_size)),
            SoftSplit(token_dim, embed_dim, stride=stride),
        )


class T2TModule(nn.Sequential):
    def __init__(
        self, in_channels, image_size, strides, token_dim, embed_dim, p_drop=0.0
    ):
        stride = strides[0]
        layers = [SoftSplit(in_channels, token_dim, stride=stride)]
        image_size = image_size // stride

        for stride in strides[1:-1]:
            layers.append(
                T2TBlock(image_size, token_dim, token_dim,
                         stride=stride, p_drop=p_drop)
            )
            image_size = image_size // stride

        stride = strides[-1]
        layers.append(
            T2TBlock(image_size, token_dim, embed_dim,
                     stride=stride, p_drop=p_drop)
        )

        super().__init__(*layers)


class TransformerBackbone(nn.Sequential):
    def __init__(self, dim, head_dim, heads, depth, mlp_mult=4, p_drop=0.0):
        layers = [
            TransformerBlock(dim, head_dim, heads, mlp_mult, p_drop)
            for _ in range(depth)
        ]
        super().__init__(*layers)


class Head(nn.Sequential):
    def __init__(self, dim, classes, p_drop=0.0):
        super().__init__(nn.LayerNorm(dim), nn.Dropout(p_drop), nn.Linear(dim, classes))


class TakeFirst(nn.Module):
    def forward(self, x):
        return x[:, 0]


class PositionEmbedding(nn.Module):
    def __init__(self, image_size, dim):
        super().__init__()
        self.pos_embedding = nn.Parameter(torch.zeros(1, image_size**2, dim))
        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))

    def forward(self, x):
        # add positional embedding
        x = x + self.pos_embedding
        # add classification token
        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)
        return x


class T2TViT(nn.Sequential):
    def __init__(
        self,
        cfg,
        strides=[2, 2, 2],
        token_dim=64,
        dim=128,
        head_dim=64,
        heads=4,
        backbone_depth=8,
        mlp_mult=2,
        in_channels=3,
        image_size=32,
        classes=10,
        trans_p_drop=0.2,
        head_p_drop=0.2,
    ):
        reduced_size = image_size // np.prod(strides)
        super().__init__(
            T2TModule(
                in_channels, image_size, strides, token_dim, dim, p_drop=trans_p_drop
            ),
            PositionEmbedding(reduced_size, dim),
            TransformerBackbone(
                dim,
                head_dim,
                heads,
                backbone_depth,
                mlp_mult=mlp_mult,
                p_drop=trans_p_drop,
            ),
            TakeFirst(),
            Head(dim, classes, p_drop=head_p_drop),
        )


---
src/visionlab/models/nets/vit.py
---
import torch.nn as nn
from einops.layers.torch import Rearrange
from visionlab.models.blocks.attentions import (
    LearnablePositionEnc,
)
from visionlab.models.blocks.composites import TransformerEncoder
from visionlab.utils.ml import init_linear
from visionlab.utils.registry import load_obj
from omegaconf import DictConfig


class ViTTiny(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()
        n_layers = 2
        embed_sz = 128
        heads = 2
        c = 3
        h = w = config.model.input_size

        self.stem = load_obj(config.model.stem.class_name)(
            in_channels=c,
            out_channels=embed_sz,
            **config.model.stem.get("params", {}),
        )

        _, _, stem_h, stem_w = self.stem.out_shape(c, h, w)

        self.trunk = nn.Sequential(
            Rearrange("n d h w -> n (h w) d"),
            LearnablePositionEnc(sizes=(stem_h * stem_w, embed_sz)),
            *[
                TransformerEncoder(
                    embed_sz,
                    num_heads=heads,
                    expansion=1,
                    dropout=0.2,
                )
                for _ in range(n_layers)
            ],
        )

        self.neck = load_obj(config.model.neck.class_name)(
            in_channels=embed_sz,
            **config.model.neck.get("params", {}),
        )

        self.head = load_obj(config.model.head.class_name)(
            in_channels=embed_sz,
            **config.model.head.params,
        )

        self.apply(init_linear)

    def forward(self, x):
        return self.head(self.neck(self.trunk(self.stem(x))))


---
src/visionlab/models/nets/convmixer.py
---
import torch.nn as nn
from einops.layers.torch import Rearrange
from visionlab.models.blocks.composites import (
    ConvMixerBlock,
)
from visionlab.utils.registry import load_obj
from omegaconf import DictConfig


class ConvMixerTiny(nn.Module):
    def __init__(self, config: DictConfig) -> None:
        super().__init__()
        n_layers = 8
        embed_sz = 256
        c = 3

        self.stem = load_obj(config.model.stem.class_name)(
            in_channels=c,
            out_channels=embed_sz,
            **config.model.stem.get("params", {}),
        )

        self.trunk = nn.Sequential(
            *[
                ConvMixerBlock(
                    embed_sz,
                    kernel_size=5,
                )
                for _ in range(n_layers)
            ],
            Rearrange("n c h w -> n (h w) c"),
        )

        self.neck = load_obj(config.model.neck.class_name)(
            in_channels=embed_sz,
            **config.model.neck.get("params", {}),
        )

        self.head = load_obj(config.model.head.class_name)(
            in_features=embed_sz,
            **config.model.head.params,
        )

    def forward(self, x):
        return self.head(self.neck(self.trunk(self.stem(x))))


---
src/visionlab/models/nets/base.py
---
import torch
from visionlab.utils.ml import init_linear
from visionlab.utils.registry import load_module
from torch import nn


class ClfModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg
        self.block = load_module(cfg.model)
        self.output_shape = self.block.output_shape
        self.apply(init_linear)

    def forward(self, x):
        return self.block(x)

    def forward_dummy(self):
        c, h, w = [self.cfg.model.ip.get(i) for i in ("c", "h", "w")]
        return self.forward(torch.randn([self.cfg.training.bs, c, h, w]))


---
src/visionlab/models/nets/ssd.py
---


---
src/visionlab/models/backbones/base.py
---
from typing import Any

import torch.nn as nn


class Backbone(nn.Module):
    def __init__(self) -> None:
        super().__init__()

    def forward(self, x: Any) -> Any:
        return x


---
src/visionlab/models/backbones/resnet.py
---
import timm
import torch.nn as nn
from visionlab.models.backbones.base import Backbone
from visionlab.utils.ml import freeze_until


class ResNetBackbone(Backbone):
    out_features: int = None

    def __init__(
        self,
        arch: str,
        pretrained: bool,
        n_layers: int = -2,
        freeze: bool = False,
        freeze_until_layer: str = None,
    ):
        super().__init__()
        if not arch.startswith("resnet"):
            raise ValueError(f"Unsupported ResNet backbone: {arch}")

        model = timm.create_model(arch, pretrained=pretrained)
        self.out_features = model.fc.in_features
        layers = list(model.children())[:n_layers]
        self.net = nn.Sequential(*layers)

        if freeze:
            freeze_until(self.net, freeze_until_layer)

    def forward(self, x):
        return self.net(x)


---
src/visionlab/lit_models/image_clf.py
---
import lightning as L
import structlog
import torch
import torch.nn as nn
from lightning.fabric.utilities.throughput import measure_flops
from visionlab.utils.registry import load_obj
from omegaconf import DictConfig

log = structlog.get_logger()


class LitImageClassifier(L.LightningModule):
    def __init__(self, cfg: DictConfig) -> None:
        super().__init__()
        self.cfg = cfg
        self.model = load_obj(cfg.model.class_name)(cfg)
        self.model.__class__.__name__ = cfg.general.model_name
        self.loss = load_obj(cfg.loss.class_name)(
            **cfg.loss.get("params", {}),
        )
        self.metrics = nn.ModuleDict()
        for metric_name, metric in cfg.metric.items():
            if metric_name.startswith("_"):
                continue
            self.metrics.update(
                {
                    metric_name: load_obj(metric.class_name)(
                        **metric.params,
                    ),
                }
            )
        self.setup(None)

    def setup(self, stage) -> None:
        with torch.device("meta"):
            model = load_obj(self.cfg.model.class_name)(self.cfg)
            self.flops = measure_flops(
                model,
                model.forward_dummy,
            )
        log.info(f"TFLOPs: {self.flops / 1e12}")

    def forward(self, x, *args, **kwargs):
        return self.model(x)

    def predict_step(self, batch, batch_idx):
        image = batch["image"]
        if image.device != self.device:
            image = image.to(self.device)
        return image, self(image).argmax(dim=-1)

    def configure_optimizers(self):
        if self.cfg.scheduler.class_name.endswith(("OneCycleLR",)):
            self.cfg.scheduler.params.total_steps = (
                self.trainer.estimated_stepping_batches
            )
        optimizer = load_obj(self.cfg.optimizer.class_name)(
            self.model.parameters(), **self.cfg.optimizer.params
        )
        scheduler = load_obj(self.cfg.scheduler.class_name)(
            optimizer, **self.cfg.scheduler.params
        )

        return (
            [optimizer],
            [
                {
                    "scheduler": scheduler,
                    "interval": self.cfg.scheduler.step,
                    "monitor": self.cfg.scheduler.monitor,
                }
            ],
        )

    def training_step(self, batch, *args, **kwargs):
        image = batch["image"]
        logits = self(image)

        target = batch["target"]
        loss = self.loss(logits, target)
        self.log(
            "train_loss",
            loss,
            on_step=False,
            on_epoch=True,
            prog_bar=True,
            logger=True,
        )

        for metric in self.metrics:
            score = self.metrics[metric](logits, target)
            self.log(
                f"train_{metric}",
                score,
                on_step=False,
                on_epoch=True,
                prog_bar=True,
                logger=True,
            )

        lr = self.trainer.lr_scheduler_configs[0].scheduler.get_last_lr()[0]
        self.log(
            "lr",
            lr,
            on_step=True,
            prog_bar=True,
            logger=True,
        )

        return loss

    def validation_step(self, batch, *args, **kwargs):
        image = batch["image"]
        logits = self(image)

        target = batch["target"]
        loss = self.loss(logits, target)

        self.log(
            "valid_loss",
            loss,
            on_epoch=True,
            prog_bar=True,
            logger=True,
        )
        for metric in self.metrics:
            score = self.metrics[metric](logits, target)
            self.log(
                f"valid_{metric}",
                score,
                on_epoch=True,
                prog_bar=True,
                logger=True,
            )

    def test_step(self, batch, *args, **kwargs):
        image = batch["image"]
        target = batch["target"]
        logits = self(image)

        for metric in self.metrics:
            score = self.metrics[metric](logits, target)
            self.log(
                f"test_{metric}",
                score,
                on_epoch=True,
                prog_bar=True,
                logger=True,
            )


---
src/visionlab/datasets/image_clf_ds.py
---
from pathlib import Path
from typing import List, Tuple

import albumentations as A
import cv2
import numpy as np
from albumentations.pytorch import ToTensorV2
from torch.utils.data import Dataset


def create_transforms():
    return A.Compose(
        [
            A.Resize(
                width=224,
                height=224,
            ),
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
            ),
            ToTensorV2(),
        ],
    )


class ImageClfDataset(Dataset):
    def __init__(
        self,
        samples: List[Tuple[Path, str]],
        mode: str = "train",
        transform=None,
        classes=None,
        class_to_idx=None,
    ):
        super().__init__()
        self.samples = samples
        self.mode = mode
        self.transform = transform
        self.classes = classes
        self.class_to_idx = class_to_idx

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        image_path, target = self.samples[idx]

        image = cv2.imread(f"{image_path}", cv2.IMREAD_COLOR)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        if image is None:
            raise FileNotFoundError(image_path)

        if self.transform is not None:
            image = self.transform(image=image)["image"]
        return {"image": image, "target": np.array(target).astype("int64")}


---
src/visionlab/datasets/tv_clf_ds.py
---
import cv2
import numpy as np
import torch
from torchvision.datasets import CIFAR10, MNIST, FashionMNIST


class TorchvisionClfDataset:
    map_converters = {
        "gray2rgb": lambda x: cv2.cvtColor(x, cv2.COLOR_GRAY2RGB),
        "tensor2npy": lambda x: x.numpy(),
        "noop": lambda x: x,
    }

    def __init__(self, dataset_name, **kwargs):
        self.dataset_name = dataset_name
        self._dataset = self._get_dataset(**kwargs)
        self.classes = self._dataset.classes
        self.class_to_idx = self._dataset.class_to_idx
        self.converters = []

        first = self._dataset.data[0]
        if isinstance(first, torch.Tensor):
            self.converters.append(self.map_converters["tensor2npy"])

        if first.ndim == 2:
            self.converters.append(self.map_converters["gray2rgb"])

    def _get_dataset(self, **kwargs):
        if self.dataset_name == "CIFAR10":
            return CIFAR10(**kwargs)
        elif self.dataset_name == "MNIST":
            return MNIST(**kwargs)
        elif self.dataset_name == "FashionMNIST":
            return FashionMNIST(**kwargs)
        else:
            raise ValueError(f"Unsupported dataset: {self.dataset_name}")

    def __getitem__(self, idx):
        image, target = self._dataset.data[idx], int(
            self._dataset.targets[idx])

        for func in self.converters:
            image = func(image)

        return {"image": image, "target": np.array(target).astype("int64")}

    def __len__(self):
        return len(self._dataset)


---
